<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on A minimal Hugo website</title><link>https://chenhaot.github.io/</link><description>Recent content in Home on A minimal Hugo website</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 19 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://chenhaot.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Hello World</title><link>https://chenhaot.github.io/blog/2025/03/19/hello-world/</link><pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/blog/2025/03/19/hello-world/</guid><description>&lt;p>Hello World!&lt;/p></description></item><item><title/><link>https://chenhaot.github.io/activities/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/activities/</guid><description>&lt;p>Title: Experiences&lt;/p>
&lt;p>&lt;strong>Education&lt;/strong>&lt;/p>
&lt;p>(2010-2016) Cornell University, Ithaca, NY&lt;br>
Ph.D. in Computer Science &lt;br>
(2006-2010) Tsinghua University, Beijing, China &lt;br>
B.Eng in Computer Science&lt;br>
B.Eng in Economics (Second degree)&lt;/p>
&lt;p>&lt;strong>Professional activities&lt;/strong>&lt;/p>
&lt;p>&lt;em>PC member&lt;/em>: WWW'2016, KDD'2016, WSDM'2016, NAACL'2016, SDM'2016, WWW'2015, KDD'2015, WSDM'2015, WWW'2014, EMNLP'2014, ICWSM'2014, DYAD'2014, IJCAI'2013, NAACL Student Research Workshop'2013, ADMA'2012, ADMA'2011. &lt;br>
&lt;em>Reviewer&lt;/em>: Transactions on Knowledge Discovery from Data (2015, 2012), CSCW'2016, WWW'2011, ICDM'2011, SIGKDD'2010, WWW'2010, CIKM'2010. &lt;br>
&lt;em>Session chair&lt;/em>: Mining and Modeling Online Activities at WWW’2015.&lt;/p></description></item><item><title/><link>https://chenhaot.github.io/awards/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/awards/</guid><description>&lt;p>Title: Awards&lt;/p>
&lt;ul>
&lt;li>Sloan research fellowship (2023)&lt;/li>
&lt;li>Outstanding Senior Area Chair at ACL (2023)&lt;/li>
&lt;li>NSF-CSIRO: HCC: Small: From Legislations to Action: Responsible AI for Climate Change (2023)&lt;/li>
&lt;li>Best Paper Award at the ICML Workshop on Human-Machine Collaboration and Teaming (2022)&lt;/li>
&lt;li>Google research scholar award (2022)&lt;/li>
&lt;li>JP Morgan faculty research award (2022)&lt;/li>
&lt;li>Data and Democracy initial grant: Partisan Polarization and Political Dysfunction (co-PI, 2022)&lt;/li>
&lt;li>Data and Democracy initial grant: Understanding and Auditing Content Moderation Policies (co-PI, 2022)&lt;/li>
&lt;li>CDAC Discovery Challenge: AI-Driven Tutorials for Radiology Student Training (lead PI, 2021)&lt;/li>
&lt;li>FAI: Towards Adaptive and Interactive Post hoc Explanations, National Science Foundation (lead PI,&lt;/li>
&lt;/ul>
&lt;ol start="2021">
&lt;li>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;a href="https://www.colorado.edu/cs/2020/04/02/bridging-gap-between-ai-and-humans">National Science Foundation CAREER award&lt;/a> (2020).&lt;/li>
&lt;li>Salesforce research award (2020).&lt;/li>
&lt;li>Amazon research award (2020).&lt;/li>
&lt;li>National Science Foundation (2019). AI-Society DCL EAGER (sole PI).&lt;/li>
&lt;li>National Science Foundation (2019). CHS Small (co-PI with Brian Keegan).&lt;/li>
&lt;li>National Science Foundation (2019). CRII (sole PI).&lt;/li>
&lt;li>National Science Foundation (2018). BIGDATA Large (co-PI with Tamara Sumner, Jennifer Jacobs, James Martin, and Wayne Ward).&lt;/li>
&lt;li>Facebook Global Literacy Challenge, final list (2018).&lt;/li>
&lt;li>Facebook (2018). Research gift.&lt;/li>
&lt;li>NVIDIA GPU grant (2017).&lt;/li>
&lt;li>University of Washington (2017). Postdoc research awards: Understand media bias through the lens of images.&lt;/li>
&lt;/ul>
&lt;h4 id="before-phd-graduation">Before PhD graduation&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://facebook.com/notes/facebook-fellowship-program/2015-2016-fellowship-winner-highlight-chenhao-tan-cornell-university/1797400167152568/">Facebook Fellowship&lt;/a> (2015)&lt;/li>
&lt;li>&lt;a href="http://www.tellurideassociation.org/our-programs/for-university-students/cornell-branch-cbta/">Telluride Residential Scholarship&lt;/a> (2012-2016)&lt;/li>
&lt;li>&lt;a href="http://labs.yahoo.com/news/691">Yahoo! Key Scientific Challenges&lt;/a> (2012)&lt;/li>
&lt;li>KDD Student Travel Award (2011, 2010)&lt;/li>
&lt;li>Best Undergraduate Thesis of Tsinghua University (Top 1 among ~160) (2010)&lt;/li>
&lt;li>Excellent Undergraduate of Tsinghua University (Top 58 among 3144) (2010)&lt;/li>
&lt;li>Excellent Undergraduate of Beijing (2010)&lt;/li>
&lt;li>National Scholarship (Top 3 among ~160) (2009, 2008)&lt;/li>
&lt;/ul></description></item><item><title>(De-)emphasis FOMC</title><link>https://chenhaot.github.io/papers/de-emphasis-fomc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/de-emphasis-fomc/</guid><description>The effect of (de-)emphasis in FOMC meetings</description></item><item><title>Bad news travel faster</title><link>https://chenhaot.github.io/papers/bad-news-travel-faster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/bad-news-travel-faster/</guid><description>&lt;h2 id="does-bad-news-go-away-faster">Does Bad News Go Away Faster?&lt;/h2>
&lt;p>&lt;a href="http://www.cs.cornell.edu/~sw475/">Shaomei Wu&lt;/a>, Chenhao Tan, &lt;a href="http://www.cs.cornell.edu/home/kleinber">Jon Kleinberg&lt;/a>, &lt;a href="http://www.soc.cornell.edu/faculty/macy.html">Michael Macy&lt;/a> &lt;br>
In Proceedings of Fifth International AAAI Conference on Weblogs and Social Media (ICWSM'2011) (short paper)&lt;/p>
&lt;p>We study the relationship between content and temporal dynamics of information on Twitter, focusing on the persistence of information. We compare two extreme temporal patterns in the decay rate of URLs embedded in tweets, defining a prediction task to distinguish between URLs that fade rapidly following their peak of popularity and those that fade more slowly. Our experiments show a strong association between the content and the temporal dynamics of information: given unigram features extracted from corresponding HTML webpages, a linear SVM classifier can predict the temporal pattern of URLs with high accuracy. We further explore the content of URLs in the two temporal classes using various textual analysis techniques (via LIWC and trend detection). We find that the rapidly-fading information contains significantly more words related to negative emotion, actions, and more complicated cognitive processes, whereas the persistent information contains more words related to positive emotion, leisure, and lifestyle.&lt;/p></description></item><item><title>bilingual joint learning</title><link>https://chenhaot.github.io/papers/bilingual-joint-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/bilingual-joint-learning/</guid><description>&lt;h2 id="joint-bilingual-sentiment-classification-with-unlabeled-parallel-corpora">Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora&lt;/h2>
&lt;p>Bin Lu, Chenhao Tan, &lt;a href="http://www.cs.cornell.edu/home/cardie/">Claire Cardie&lt;/a>, Benjamin K. Tsou &lt;br>
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL'2011)&lt;/p>
&lt;p>Most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource-rich languages to resource-poor languages. We present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data. We rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines.&lt;/p></description></item><item><title>Biography</title><link>https://chenhaot.github.io/bio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/bio/</guid><description>a short history of my life.</description></item><item><title>bots sense of virtual community</title><link>https://chenhaot.github.io/papers/bots-sense-of-virtual-community/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/bots-sense-of-virtual-community/</guid><description>&lt;h2 id="the-impact-of-governance-bots-on-sense-of-virtual-community-development-and-validation-of-the-gov-bots-scale">The Impact of Governance Bots on Sense of Virtual Community: Development and Validation of the GOV-BOTs Scale&lt;/h2>
&lt;p>&lt;a href="https://estellesmithphd.com">C. Estelle Smith&lt;/a>, Irfanul Alam, &lt;em>Chenhao Tan&lt;/em>, &lt;a href="https://www.brianckeegan.com/">Brian C. Keegan&lt;/a>, &lt;a href="https://pages.charlotte.edu/anitablanchard/">Anita Blanchard&lt;/a> &lt;br>
In Proceedings of CSCW 2022.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Bots are increasingly being used for governance-related purposes in online communities, yet no instrumentation exists for measuring how users assess their beneficial or detrimental impacts. In order to support future human-centered and community-based research, we developed a new scale called GOVernance Bots in Online communiTies (GOV-BOTs) across two rounds of surveys on Reddit (N = 820). We applied rigorous psychometric criteria to demonstrate the validity of GOV-BOTs, which contains two subscales: bot governance (4 items) and bot tensions (3 items). Whereas humans have historically expected communities to be composed entirely of humans, the social participation of bots as non-human agents now raises fundamental questions about psychological, philosophical, and ethical implications. Addressing psychological impacts, our data show that perceptions of effective bot governance positively contribute to users’ sense of virtual community (SOVC), whereas perceived bot tensions may only impact SOVC if users are more aware of bots. Finally, we show that users tend to experience the greatest SOVC across groups of subreddits, rather than individual subreddits, suggesting that future research should carefully re-consider uses and operationalizations of the term “community.”&lt;/p></description></item><item><title>changemyview</title><link>https://chenhaot.github.io/papers/changemyview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/changemyview/</guid><description>A study on persuasion</description></item><item><title>Collaborators</title><link>https://chenhaot.github.io/collaborators/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/collaborators/</guid><description>&lt;ul>
&lt;li>&lt;a href="http://www.ladamic.com/">Lada Adamic&lt;/a>&lt;/li>
&lt;li>David Atkinson&lt;/li>
&lt;li>Irfanul Alam&lt;/li>
&lt;li>&lt;a href="https://homes.cs.washington.edu/~taugust/">Tal August&lt;/a>&lt;/li>
&lt;li>Rajat Bhatnagar&lt;/li>
&lt;li>&lt;a href="https://pages.charlotte.edu/anitablanchard/">Anita Blanchard&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://wbrackenbury.github.io">Will Brackenbury&lt;/a>&lt;/li>
&lt;li>Erin Bransom&lt;/li>
&lt;li>&lt;a href="https://cs.uchicago.edu/people/kevin-bryson/">Kevin Bryson&lt;/a>&lt;/li>
&lt;li>Kwam Byll&lt;/li>
&lt;li>&lt;a href="https://users.umiacs.umd.edu/~jbg/">Jordan Boyd-Graber&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.cs.cornell.edu/home/cardie/">Claire Cardie&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://joncaizheng.com/">Jon Z. Cai&lt;/a>&lt;/li>
&lt;li>Tianyu Cao&lt;/li>
&lt;li>Milo Chase&lt;/li>
&lt;li>&lt;a href="https://chacha-chen.github.io">Chacha Chen&lt;/a>&lt;/li>
&lt;li>Qian Chen&lt;/li>
&lt;li>&lt;a href="https://yuxinchen.org">Yuxin Chen&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www-users.cs.umn.edu/~echi/">Ed Chi&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://homes.cs.washington.edu/~eunsol/home.html">Eunsol Choi&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://homes.cs.washington.edu/~eaclark7/">Elizabeth Clark&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.cs.cmu.edu/~dcard/">Dallas Card&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://shcarton.github.io/">Samuel Carton&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://sites.google.com/site/tiagocunha87/">Tiago Cunha&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.cs.cornell.edu/~cristian/">Cristian Danescu-Niculescu-Mizil&lt;/a>&lt;/li>
&lt;li>Danial Dervovic&lt;/li>
&lt;li>Solomon Dworkin&lt;/li>
&lt;li>&lt;a href="https://people.cs.uchicago.edu/~aelmore/">Aaron J. Elmore&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://u.osu.edu/erel-koksal.1/">Isil Erel&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.shifeng.umiacs.io">Shi Feng&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://raulcastrofernandez.com/">Raul Castro Fernandez&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cs.uchicago.edu/people/michael-franklin/">Michael J. Franklin&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.friggeri.net/">Adrien Friggeri&lt;/a>&lt;/li>
&lt;li>Zhengzhe Fu&lt;/li>
&lt;li>&lt;a href="http://www.cs.technion.ac.il/~gabr/">Evgeniy Gabrilovich&lt;/a>&lt;/li>
&lt;li>Bo Gao&lt;/li>
&lt;li>&lt;a href="https://xhan77.github.io/">Xiaochuang Han&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://hhexiy.github.io/">He He&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://people.cs.uchicago.edu/~mourad/">Mourad Heddaya&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.cs.cornell.edu/~jhessel/">Jack Hessel&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://chaochunhsu.github.io">Chao-Chun Hsu&lt;/a>&lt;/li>
&lt;li>William Howell&lt;/li>
&lt;li>&lt;a href="http://www.davehuffaker.com">David Huffaker&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://jiyfeng.github.io/">Yangfeng Ji&lt;/a>&lt;/li>
&lt;li>Long Jiang&lt;/li>
&lt;li>&lt;a href="https://njjiang.github.io/">Nan-Jiang Jiang&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://jurgens.people.si.umich.edu/">David Jurgens&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://surya-kanoria.github.io">Surya Kanoria&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.brianckeegan.com/">Brian Keegan&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kiciman.org/">Emre Kıcıman&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.cs.cornell.edu/home/kleinber">Jon Kleinberg&lt;/a>&lt;/li>
&lt;li>Isabel Kloumann&lt;/li>
&lt;li>Arnd Christian Konig&lt;/li>
&lt;li>&lt;a href="https://sites.google.com/site/gkossinets/">Gueorgi Kossinets&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://sanjayk.io/">Sanjay Krishnan&lt;/a>&lt;/li>
&lt;li>Bailey Kuehl&lt;/li>
&lt;li>&lt;a href="https://vivlai.github.io/">Vivian Lai&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://himalakkaraju.github.io">Himabindu Lakkaraju&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.cs.cornell.edu/home/llee">Lillian Lee&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://people.csail.mit.edu/taolei/">Tao Lei&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://levyomer.wordpress.com/">Omer Levy&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.stat.cornell.edu/~li/">Ping Li&lt;/a>&lt;/li>
&lt;li>Tian Li&lt;/li>
&lt;li>Yafeng Li&lt;/li>
&lt;li>&lt;a href="http://qveraliao.com">Q. Vera Liao&lt;/a>&lt;/li>
&lt;li>Quan Lin&lt;/li>
&lt;li>Yinghe Lin&lt;/li>
&lt;li>&lt;a href="https://www.littmania.com">Michael Littman&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mrsata.github.io/">Han Liu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://laoliu5280.github.io/">Haokun Liu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://homes.cs.washington.edu/~nfliu/">Nelson F. Liu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://tracyxliu.com/">Tracy Xiao Liu&lt;/a>&lt;/li>
&lt;li>Zongtao Liu&lt;/li>
&lt;li>&lt;a href="https://cognition.princeton.edu/people/tania-lombrozo">Tania Lombrozo&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://sites.google.com/site/lubin2010/">Bin Lu&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://blubars.github.io/pages/about.html">Brian Lubars&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kel-lu.github.io">Kelvin Luu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.cs.colorado.edu/~lv/">Qin Lv&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://madhu-aithal.github.io/">Madhusudhan Aithal Mahabhaleshwara&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.soc.cornell.edu/faculty/macy.html">Michael Macy&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://divy.at/">Divyat Mahajan&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.asc.ohio-state.edu/demarneffe.1/">Marie-Catherine de Marneffe&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://smartschat.de/">Sebastian Martschat&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.mcnutt.in">Andrew McNutt&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.hongyuanmei.com/">Hongyuan Mei&lt;/a>&lt;/li>
&lt;li>Alexander A. Meitus&lt;/li>
&lt;li>&lt;a href="https://raam93.github.io/">Ramaravind K. Mothilal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://sendhil.org/">Sendhil Mullainathan&lt;/a>&lt;/li>
&lt;li>Anne Mykland&lt;/li>
&lt;li>&lt;a href="https://www.cs.cmu.edu/~anaik/">Aakanksha Naik&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.linkedin.com/in/osazuwa/">Robert Ness&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://dangng2004.github.io/">Dang Nguyen&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://vene.ro/">Vlad Niculae&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://ziadobermeyer.com/">Ziad Obermeyer&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://sites.google.com/view/nigini/">Nigini Oliveira&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://sites.google.com/site/bopang42/">Bo Pang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://homes.cs.washington.edu/~hapeng/">Hao Peng&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.media.mit.edu/people/sandy/overview/">Alex &amp;lsquo;Sandy&amp;rsquo; Pentland&lt;/a>&lt;/li>
&lt;li>Gale H. Prinster&lt;/li>
&lt;li>Natraj Raman&lt;/li>
&lt;li>&lt;a href="https://rathoreanirudh.github.io/">Anirudh Rathore&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://homes.cs.washington.edu/~reinecke/index.html">Katharina Reinecke&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.dromero.org/">Daniel M. Romero&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://homes.cs.washington.edu/~ansross/">Anne Ross&lt;/a>&lt;/li>
&lt;li>Xiaoming Shi&lt;/li>
&lt;li>Jenna Sparks&lt;/li>
&lt;li>&lt;a href="https://leastern.com/">Lea Stern&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://homes.cs.washington.edu/~roysch/">Roy Schwartz&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.amitsharma.in/">Amit Sharma&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://sameersingh.org">Sameer Singh&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://dylanslacks.website">Dylan Slack&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://estellesmithphd.com">Estelle Smith&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://homes.cs.washington.edu/~nasmith/">Noah A. Smith&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://alisonmsmith.github.io">Alison Smith-Renner&lt;/a>&lt;/li>
&lt;li>&lt;a href="alex.smola.org">Alex Smola&lt;/a>&lt;/li>
&lt;li>Tejes Srivastava&lt;/li>
&lt;li>&lt;a href="https://leastern.com/">Léa H. Stern&lt;/a>&lt;/li>
&lt;li>Kumar Bhargav Srinivasan&lt;/li>
&lt;li>Jamar L. Sullivan&lt;/li>
&lt;li>&lt;a href="http://www.sunlab.org/">Jimeng Sun&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://keg.cs.tsinghua.edu.cn/persons/johan_ugander">Jie Tang&lt;/a>&lt;/li>
&lt;li>Wenbin Tang&lt;/li>
&lt;li>Benjamin K. Tsou&lt;/li>
&lt;li>&lt;a href="http://people.cam.cornell.edu/~jugander/">Johan Ugander&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://blaseur.com">Blase Ur&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://faculty.wcas.northwestern.edu/robvoigt/">Rob Voigt&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://dwadden.github.io/">David Wadden&lt;/a>&lt;/li>
&lt;li>Fengjiao Wang&lt;/li>
&lt;li>Grace Wang&lt;/li>
&lt;li>Kangrui Wang&lt;/li>
&lt;li>&lt;a href="https://llwang.net/">Lucy Lu Wang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://u.osu.edu/weisbach.2/">Michael S. Weisbach&lt;/a>&lt;/li>
&lt;li>Fei Wu&lt;/li>
&lt;li>&lt;a href="http://www.cs.cornell.edu/~sw475/">Shaomei Wu&lt;/a>&lt;/li>
&lt;li>Siqiao Xue&lt;/li>
&lt;li>Xuan Yang&lt;/li>
&lt;li>&lt;a href="http://yangy.org/">Yang Yang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.alexanderkzentefis.com/">Alexander Zentefis&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.jasondarkblue.com/">Jason Shuo Zhang&lt;/a>&lt;/li>
&lt;li>James Y. Zhang&lt;/li>
&lt;li>&lt;a href="https://y0mingzhang.github.io/">Yiming Zhang&lt;/a>&lt;/li>
&lt;li>Yunfeng Zhang&lt;/li>
&lt;li>Fan Zhou&lt;/li>
&lt;li>Jun Zhou&lt;/li>
&lt;li>&lt;a href="https://karen-zhou.com/">Karen Zhou&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://research.microsoft.com/en-us/people/mingzhou">Ming Zhou&lt;/a>&lt;/li>
&lt;li>Yueting Zhuang&lt;/li>
&lt;li>Thomas Zimmermann&lt;/li>
&lt;/ul></description></item><item><title>community genealogy</title><link>https://chenhaot.github.io/papers/community-genealogy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/community-genealogy/</guid><description>&lt;h2 id="tracing-community-genealogy-how-new-communities-emerge-from-the-old">Tracing Community Genealogy: How New Communities Emerge from the Old&lt;/h2>
&lt;p>Chenhao Tan. &lt;br>
In Proceedings of the 12th International AAAI Conference on Web and Social Media (ICWSM'2018).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
The process by which new communities emerge is a central research issue in
the social sciences. While a growing body of research analyzes the formation of
a single community by examining social networks between individuals, we
introduce a novel community-centered perspective. We highlight the fact that
the context in which a new community emerges contains numerous existing
communities. We reveal the emerging process of communities by tracing their early members&amp;rsquo; previous community memberships.&lt;/p></description></item><item><title>community success</title><link>https://chenhaot.github.io/papers/community-success/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/community-success/</guid><description>&lt;h2 id="are-all-successful-communities-alike-characterizing-and-predicting-the-success-of-online-communities">Are All Successful Communities Alike? Characterizing and Predicting the Success of Online Communities&lt;/h2>
&lt;p>&lt;a href="https://sites.google.com/site/tiagocunha87/">Tiago Cunha&lt;/a>, &lt;a href="http://jurgens.people.si.umich.edu/">David Jurgens&lt;/a>, &lt;em>Chenhao Tan&lt;/em> and &lt;a href="http://www.dromero.org/">Daniel Romero&lt;/a>. &lt;br>
In Proceedings of The Web Conference (WWW'2019).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
The proliferation of online communities has created exciting opportunities to study the mechanisms that explain group success. While a growing body of research investigates community success through a single measure — typically, the number of members — we argue that there are multiple ways of measuring success. Here, we present a systematic study to understand the relations between these success definitions and test how well they can be predicted based on community properties and behaviors from the earliest period of a community’s lifetime. We identify four success measures
that are desirable for most communities: (i) growth in the number of members; (ii) retention of members; (iii) long term survival of the community; and (iv) volume of activities within the community. Surprisingly, we find that our measures do not exhibit very high correlations, suggesting that they capture different types of
success. Additionally, we find that different success measures are predicted by different attributes of online communities, suggesting that success can be achieved through different behaviors. Our work sheds light on the basic understanding on what success represents in online communities and what predicts it. Our results suggest
that success is multi-faceted and cannot be measured nor predicted by a single measurement. This insight has practical implications for the creation of new online communities and the design of platforms that facilitate such communities.&lt;/p></description></item><item><title>conditional delegation</title><link>https://chenhaot.github.io/papers/conditional-delegation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/conditional-delegation/</guid><description>&lt;h2 id="human-ai-collaboration-via-conditional-delegation-a-case-study-of-content-moderation">Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation&lt;/h2>
&lt;p>&lt;a href="https://vivlai.github.io/">Vivian Lai&lt;/a>, &lt;a href="https://shcarton.github.io/">Samuel Carton&lt;/a>, Rajat Bhatnagar, &lt;a href="http://qveraliao.com">Q. Vera Liao&lt;/a>, Yunfeng Zhang, and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Proceedings of CHI 2022.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Despite impressive performance in many benchmark datasets, AI models can still make mistakes, especially among out-of-distribution examples. It remains an open question how such imperfect models can be used effectively in collaboration with humans. Prior work has focused on AI assistance that helps people make individual high-stakes decisions, which is not scalable for a large amount of relatively low-stakes decisions, e.g., moderating social media comments. Instead, we propose conditional delegation as an alternative paradigm for human-AI collaboration where humans create rules to indicate trustworthy regions of a model. Using content moderation as a testbed, we develop novel interfaces to assist humans in creating conditional delegation rules and conduct a randomized experiment with two datasets to simulate in-distribution and out-of-distribution scenarios. Our study demonstrates the promise of conditional delegation in improving model performance and provides insights into design for this novel paradigm, including the effect of AI explanations.&lt;/p></description></item><item><title>Contact</title><link>https://chenhaot.github.io/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/contact/</guid><description>&lt;p>Chenhao &lt;em>Tan&lt;/em> (&lt;em>谭&lt;/em> 宸浩)&lt;br>
Searle 215&lt;br>
5735 S Ellis Ave&lt;br>
Chicago, IL 60637&lt;br>
Email: [firstname] [AT] uchicago.edu; [firstname] [AT] chenhaot.com (please do not send teaching related emails to this address)&lt;/p></description></item><item><title>content removal</title><link>https://chenhaot.github.io/papers/content-removal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/content-removal/</guid><description>&lt;h2 id="content-removal-as-a-moderation-strategy-compliance-and-other-outcomes-in-the-changemyview-community">Content removal as a moderation strategy: Compliance and other outcomes in the ChangeMyView community&lt;/h2>
&lt;p>Kumar Bhargav Srinivasan, &lt;a href="http://www.cs.cornell.edu/~cristian/">Cristian Danescu-Niculescu-Mizil&lt;/a>, &lt;a href="http://www.cs.cornell.edu/home/llee">Lillian Lee&lt;/a>, and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Proceedings of the Conference on Computer-Supported Cooperative Work and Social Computing (CSCW'2019).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Moderators of online communities often employ comment deletion as a tool. We ask here whether, beyond the positive effects of shielding a community from undesirable content, does comment removal actually cause the behavior of the comment’s author to improve? We examine this question in a particularly well-moderated community, the ChangeMyView subreddit.
The standard analytic approach of interrupted time-series analysis unfortunately cannot answer this question of causality because it fails to distinguish the effect of having made a non-compliant comment from the effect of being subjected to moderator removal of that comment. We therefore leverage a “delayed feedback” approach based on the observation that some users may remain active between the time when they posted the non-compliant comment and the time when that comment is deleted. Applying this approach to such users, we reveal the causal role of comment deletion in reducing immediate noncompliance rates, although we do not find evidence of it having a causal role in inducing other behavior improvements. Our work thus empirically demonstrates both the promise and some potential limits of content removal as a positive moderation strategy, and points to future directions for identifying causal effects from observational data.&lt;/p></description></item><item><title>Courses</title><link>https://chenhaot.github.io/courses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/courses/</guid><description>&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/ChicagoHAI/human-centered-machine-learning">Human-Centered Machine Learning&lt;/a> (Spring 2021) at the University of Chicago&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://canvas.uchicago.edu/courses/32962">Natural Language Processing&lt;/a> (Winter 2021) at the University of Chicago&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/BoulderDS/CSCI-4622-Machine-Learning-fa20/">Undergraduate machine learning&lt;/a> (Fall 2020) at the University of Colorado Boulder&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/BoulderDS/human-centered-machine-learning">Human-centered Machine Learning&lt;/a> (Spring 2020) at the Unviersity of Colorado Boulder&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/BoulderDS/CSCI5622-Machine-Learning">Machine Learning&lt;/a> (Fall 2019) at the University of Colorado Boulder&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/BoulderDS/CSCI-4622-Machine-Learning-18fa">Undergraduate Machine Learning&lt;/a> (Fall 2018) at the University of Colorado Boulder&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="courses/hcml/home.html">Human-centered Machine Learning&lt;/a> (Spring 2018) at the University of Colorado Boulder&lt;/p></description></item><item><title>covid communities diverging trajectories</title><link>https://chenhaot.github.io/papers/covid-communities-diverging-trajectories/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/covid-communities-diverging-trajectories/</guid><description>&lt;h2 id="understanding-the-diverging-user-trajectories-in-highly-related-online-communities-during-the-covid-19-pandemic">Understanding the Diverging User Trajectories in Highly-related Online Communities during the COVID-19 Pandemic&lt;/h2>
&lt;p>&lt;a href="http://www.jasondarkblue.com/">Jason Shuo Zhang&lt;/a>, &lt;a href="https://www.cs.colorado.edu/~lv/">Qin Lv&lt;/a>, &lt;a href="https://www.brianckeegan.com/">Brian Keegan&lt;/a>, and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Proceedings of the 15th International Conference On Web and Social Media (ICWSM'2021).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
As the COVID-19 pandemic is disrupting life worldwide,
related online communities are popping up. In particular,
two “new” communities, /r/China flu and /r/Coronavirus,
emerged on Reddit and have been dedicated to COVIDrelated discussions from the very beginning of this pandemic.
With /r/Coronavirus promoted as the official community on
Reddit, it remains an open question how users choose between these two highly-related communities.&lt;br>
In this paper, we characterize user trajectories in these two
communities from the beginning of COVID-19 to the end
of September 2020. We show that new users of /r/China flu
and /r/Coronavirus were similar from January to March. After that, their differences steadily increase, evidenced by both
language distance and membership prediction, as the pandemic continues to unfold. Furthermore, users who started at
/r/China flu from January to March were more likely to leave,
while those who started in later months tend to remain highly
“loyal”. To understand this difference, we develop a movement analysis framework to understand membership changes
in these two communities and identify a significant proportion of /r/China flu members (around 50%) that moved to
/r/Coronavirus in February. This movement turns out to be
highly predictable based on other subreddits that users were
previously active in. Our work demonstrates how two highlyrelated communities emerge and develop their own identity
in a crisis, and highlights the important role of existing communities in understanding such an emergence.&lt;/p></description></item><item><title>creative writing with a machine in the loop</title><link>https://chenhaot.github.io/papers/creative-writing-with-a-machine-in-the-loop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/creative-writing-with-a-machine-in-the-loop/</guid><description>&lt;h2 id="creative-writing-with-a-machine-in-the-loop-case-studies-on-slogans-and-stories">Creative Writing with a Machine in the Loop: Case Studies on Slogans and Stories&lt;/h2>
&lt;p>&lt;a href="https://homes.cs.washington.edu/~eaclark7/">Elizabeth Clark&lt;/a>, &lt;a href="https://homes.cs.washington.edu/~ansross/">Anne Ross&lt;/a>, Chenhao Tan, &lt;a href="http://jiyfeng.github.io/">Yangfeng Ji&lt;/a>, and &lt;a href="http://homes.cs.washington.edu/~nasmith/">Noah A. Smith&lt;/a>. &lt;br>
In Proceedings of the 23rd ACM Conference on Intelligent User Interfaces (IUI’2018).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
As the quality of natural language generated by artifcial intelligence systems improves, writing interfaces can support interventions beyond grammar-checking and spell-checking, such as suggesting content to spark new ideas. To explore the possibility of machine-in-the-loop creative writing, we performed two case studies using two system prototypes, one for short story writing and one for slogan writing. Participants in our studies were asked to write with a machine in the loop or alone (control condition). They assessed their writing and experience through surveys and an open-ended interview. We collected additional assessments of the writing from Amazon Mechanical Turk crowdworkers. Our findings indicate that participants found the process fun and helpful and could envision use cases for future systems. At the same time, machine suggestions do not necessarily lead to better written artifacts. We therefore suggest novel natural language models and design choices that may better support creative writing.&lt;/p></description></item><item><title>debate quotes</title><link>https://chenhaot.github.io/papers/debate-quotes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/debate-quotes/</guid><description>&lt;h2 id="you-are-no-jack-kennedy-on-media-selection-of-highlights-from-presidential-debates">&amp;ldquo;You are no Jack Kennedy&amp;rdquo;: On Media Selection of Highlights from Presidential Debates&lt;/h2>
&lt;p>Chenhao Tan, &lt;a href="https://homes.cs.washington.edu/~hapeng/">Hao Peng&lt;/a>, and &lt;a href="http://homes.cs.washington.edu/~nasmith/">Noah A. Smith&lt;/a>. &lt;br>
In Proceedings of The Web Conference (WWW'2018).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Political speeches and debates play an important role in shaping the images of politicians, and the public often relies on media outlets to select bits of political communication from a large pool of utterances. It is an important research question to understand what factors impact this selection process.&lt;/p></description></item><item><title>decision focused summarization</title><link>https://chenhaot.github.io/papers/decision-focused-summarization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/decision-focused-summarization/</guid><description>&lt;h2 id="decision-focused-summarization">Decision-focused Summarization&lt;/h2>
&lt;p>&lt;a href="https://chaochunhsu.github.io">Chao-Chun Hsu&lt;/a> and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Proceedings of EMNLP 2021.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Relevance in summarization is typically defined based on textual information alone, without incorporating insights about a particular decision. As a result, to support risk analysis of pancreatic cancer, summaries of medical notes may include irrelevant information such as a knee injury. We propose a novel problem, decision-focused summarization, where the goal is to summarize relevant information for a decision. We leverage a predictive model that makes the decision based on the full text to provide valuable insights on how a decision can be inferred from text. To build a summary, we then select representative sentences that lead to similar model decisions as using the full text while accounting for textual non-redundancy. To evaluate our method (DecSum), we build a testbed where the task is to summarize the first ten reviews of a restaurant in support of predicting its future rating on Yelp. DecSum substantially outperforms text-only summarization methods and model-based explanation methods in decision faithfulness and representativeness. We further demonstrate that DecSum is the only method that enables humans to outperform random chance in predicting which restaurant will be better rated in the future.&lt;/p></description></item><item><title>delegability</title><link>https://chenhaot.github.io/papers/delegability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/delegability/</guid><description>&lt;h2 id="ask-not-what-ai-can-do-but-what-ai-should-do-towards-a-framework-of-task-delegability">Ask Not What AI Can Do, But What AI Should Do: Towards a Framework of Task Delegability&lt;/h2>
&lt;p>&lt;a href="http://blubars.github.io/pages/about.html">Brian Lubars&lt;/a> and Chenhao Tan. &lt;br>
In Proceedings of Thirty-third Conference on Neural Information Processing Systems (NeurIPS'2019) (spotlight presentation).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Although artificial intelligence holds promise for addressing societal challenges, issues of exactly which tasks to automate and the extent to do so remain understudied. We approach the problem of task delegability from a human-centered perspective by developing a framework on human perception of task delegation to artificial intelligence. We consider four high-level factors that can contribute to a delegation decision: motivation, difficulty, risk, and trust. To obtain an empirical understanding of human preferences in different tasks, we build a dataset of 100 tasks from academic papers, popular media portrayal of AI, and everyday life. For each task, we administer a survey to collect judgments of each factor and ask subjects to pick the extent to which they prefer AI involvement. We find little preference for full AI control and a strong preference for machine-in-the-loop designs, in which humans play the leading role. Our framework can effectively predict human preferences in degrees of AI assistance. Among the four factors, trust is the most predictive of human preferences of optimal human-machine delegation. This framework represents a first step towards characterizing human preferences of automation across tasks. We hope this work may encourage and aid in future efforts towards understanding such individual attitudes; our goal is to inform the public and the AI research community rather than dictating any direction in technology development.&lt;/p></description></item><item><title>diverse counterfactual explanations</title><link>https://chenhaot.github.io/papers/diverse-counterfactual-explanations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/diverse-counterfactual-explanations/</guid><description>&lt;h2 id="explaining-machine-learning-classifiers-through-diverse-counterfactual-explanations">Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations&lt;/h2>
&lt;p>Ramaravind Kommiya Mothilal, &lt;a href="http://www.amitsharma.in/">Amit Sharma&lt;/a> and Chenhao Tan. &lt;br>
In Proceedings of ACM FAT* Conference 2020.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at &lt;a href="https://github.com/microsoft/DiCE">https://github.com/microsoft/DiCE&lt;/a>.&lt;/p></description></item><item><title>diversity limits human explanations</title><link>https://chenhaot.github.io/papers/diversity-limits-human-explanations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/diversity-limits-human-explanations/</guid><description>&lt;h2 id="on-the-diversity-and-limits-of-human-explanations">On the Diversity and Limits of Human Explanations&lt;/h2>
&lt;p>&lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Proceedings of NAACL 2022.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
A growing effort in NLP aims to build datasets of human explanations. However, it remains unclear whether these datasets serve their intended goals. This problem is exacerbated by the fact that the term explanation is overloaded and refers to a broad range of notions with different properties and ramifications. Our goal is to provide an overview of the diversity of explanations, discuss human limitations in providing explanations, and ultimately provide implications for collecting and using human explanations in NLP.&lt;/p></description></item><item><title>entity lm</title><link>https://chenhaot.github.io/papers/entity-lm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/entity-lm/</guid><description>&lt;h2 id="dynamic-entity-representations-in-neural-language-models">Dynamic Entity Representations in Neural Language Models&lt;/h2>
&lt;p>&lt;em>&lt;a href="http://jiyfeng.github.io/">Yangfeng Ji&lt;/a>, Chenhao Tan, &lt;a href="http://smartschat.de/">Sebastian Martschat&lt;/a>, &lt;a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi&lt;/a>, &lt;a href="http://homes.cs.washington.edu/~nasmith/">Noah A. Smith&lt;/a>&lt;/em> &lt;br>
In Proceedings of EMNLP 2017&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, ENTITYNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.&lt;/p></description></item><item><title>evaluate human rationales</title><link>https://chenhaot.github.io/papers/evaluate-human-rationales/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/evaluate-human-rationales/</guid><description>&lt;h2 id="evaluating-and-characterizing-human-rationales">Evaluating and Characterizing Human Rationales&lt;/h2>
&lt;p>&lt;a href="https://shcarton.github.io/">Samuel Carton&lt;/a>, &lt;a href="https://rathoreanirudh.github.io/">Anirudh Rathore&lt;/a>, and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP'2020)&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using &amp;ldquo;fidelity curves&amp;rdquo; to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.&lt;/p></description></item><item><title>Expertise matching</title><link>https://chenhaot.github.io/papers/expertise-matching/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/expertise-matching/</guid><description>&lt;h2 id="on-optimization-of-expertise-matching-with-various-constraints">On optimization of expertise matching with various constraints&lt;/h2>
&lt;p>Wenbin Tang, &lt;a href="http://keg.cs.tsinghua.edu.cn/persons/johan_ugander">Jie Tang&lt;/a>, &lt;a href="http://people.csail.mit.edu/taolei/">Tao Lei&lt;/a>, Chenhao Tan, Bo Gao, Tian Li &lt;br>
In Neurocomputing (January, 2012)&lt;/p>
&lt;p>This paper studies the problem of expertise matching with various constraints. Expertise matching, which aims to find the alignment between experts and queries, is a common problem in many applications such as conference paper-reviewer assignment, productreviewer alignment, and product-endorser matching. Most existing methods formalize this problem as an information retrieval problem and focus on finding a set of experts for each query independently. However, in real-world systems, various constraints are often needed to be considered. For example, in order to review a paper, it is desirable that there is at least one senior reviewer to guide the reviewing process. An important question is: “Can we design a framework to efficiently find the optimal solution for expertise matching under various constraints?” This paper explores such an approach by formulating the expertise matching problem in a constrain-based optimization framework. In the proposed framework, the problem of expertise matching is linked to a convex cost flow problem, which guarantees an optimal solution under various constraints. We also present an online matching algorithm to support incorporating user feedbacks in real time. The proposed approach has been evaluated on two different genres of expertise matching problems, namely conference paper-reviewer assignment and teacher-course assignment. Experimental results validate the effectiveness of the proposed approach. Based on the proposed method, we have also developed an online system for paper-reviewer suggestions, which has been used for paper-reviewer assignment in a top conference and feedbacks from the conference organizers are very positive.&lt;/p></description></item><item><title>explanation few-shot NLP</title><link>https://chenhaot.github.io/papers/explanation-few-shot-nlp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/explanation-few-shot-nlp/</guid><description>&lt;h2 id="investigating-the-effect-of-natural-language-explanations-on-out-of-distribution-generalization-in-few-shot-nli">Investigating the Effect of Natural Language Explanations on Out-of-Distribution Generalization in Few-shot NLI&lt;/h2>
&lt;p>&lt;a href="https://rosafish.github.io">Yangqiaoyu Zhou&lt;/a> and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Workshop on Insights from Negative Results in NLP at EMNLP 2021.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Although neural models have shown strong performance in datasets such as SNLI, they lack the ability to generalize out-of-distribution (OOD). In this work, we formulate a few-shot learning setup and examine the effects of natural language explanations on OOD generalization. We leverage the templates in the HANS dataset and construct templated natural language explanations for each template. Although generated explanations show competitive BLEU scores against groundtruth explanations, they fail to improve prediction performance. We further show that generated explanations often hallucinate information and miss key elements that indicate the label.&lt;/p></description></item><item><title>explanation pointers</title><link>https://chenhaot.github.io/papers/explanation-pointers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/explanation-pointers/</guid><description>&lt;h2 id="what-gets-echoed-understanding-the-pointers-in-explanations-of-persuasive-arguments">What Gets Echoed? Understanding the &amp;lsquo;Pointers&amp;rsquo; in Explanations of Persuasive Arguments&lt;/h2>
&lt;p>David Atkinson, Kumar Bhargav Srinivasan, and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Proceedings of 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP'2019)&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Explanations are central to everyday life, and are a topic of growing interest in the AI community. To investigate the process of providing natural language explanations, we leverage the dynamics of the /r/ChangeMyView subreddit to build a dataset with 36K naturally occurring explanations of why an argument is persuasive. We propose a novel word-level prediction task to investigate how explanations selectively reuse, or echo, information from what is being explained (henceforth, explanandum). We develop features to capture the properties of a word in the explanandum, and show that our proposed features not only have relatively strong predictive power on the echoing of a word in an explanation, but also enhance neural methods of generating explanations. In particular, while the non-contextual properties of a word itself are more valuable for stopwords, the interaction between the constituent parts of an explanandum is crucial in predicting the echoing of content words. We also find intriguing patterns of a word being echoed. For example, although nouns are generally less likely to be echoed, subjects and objects can, depending on their source, be more likely to be echoed in the explanations.&lt;/p></description></item><item><title>FAQ</title><link>https://chenhaot.github.io/faq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/faq/</guid><description>&lt;p>&lt;strong>I want to work with you, and I&amp;rsquo;m not currently a student at the University of Chicago. How do I work with you?&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>If you are considering doing a postdoc with me, please email me with your CV, two names of references, and an optional one-page research statement. All the postdocs I have supervised so far have successfully landed tenure-track positions at R1 universities (check out &lt;a href="https://chicagohai.github.io/">my group website&lt;/a>), and I always have openings for talented postdocs.&lt;/p></description></item><item><title>feature importance</title><link>https://chenhaot.github.io/papers/feature-importance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/feature-importance/</guid><description>&lt;h2 id="many-faces-of-feature-importance-comparing-built-in-and-post-hoc-feature-importance-in-text-classification">Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification&lt;/h2>
&lt;p>&lt;a href="https://vivlai.github.io/">Vivian Lai&lt;/a>, &lt;a href="https://joncaizheng.com/">Jon Z. Cai&lt;/a>, and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Proceedings of 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP'2019)&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Feature importance is commonly used to explain machine predictions. While feature importance can be derived from a machine learning model with a variety of methods, the consistency of feature importance via different methods remains understudied. In this work, we systematically compare feature importance from built-in mechanisms in a model such as attention values and post-hoc methods that approximate model behavior such as LIME. Using text classification as a testbed, we find that 1) no matter which method we use, important features from traditional models such as SVM and XGBoost are more similar with each other, than with deep learning models; 2) post-hoc methods tend to generate more similar important features for two models than built-in methods. We further demonstrate how such similarity varies across instances. Notably, important features do not always resemble each other better when two models agree on the predicted label than when they disagree.&lt;/p></description></item><item><title>framing sample</title><link>https://chenhaot.github.io/papers/framing-sample/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/framing-sample/</guid><description>&lt;h2 id="framing-effect-the-choice-of-slogans-used-to-advertise-online-experiments-can-boost-recruitment-and-lead-to-sample-biases">Framing Effect: The Choice of Slogans Used to Advertise Online Experiments Can Boost Recruitment and Lead to Sample Biases&lt;/h2>
&lt;p>&lt;a href="https://homes.cs.washington.edu/~taugust/">Tal August&lt;/a>, &lt;a href="https://sites.google.com/view/nigini/">Nigini Oliveira&lt;/a>, &lt;em>Chenhao Tan&lt;/em>, &lt;a href="http://homes.cs.washington.edu/~nasmith/">Noah A. Smith&lt;/a>, and &lt;a href="https://homes.cs.washington.edu/~reinecke/index.html">Katharina Reinecke&lt;/a>. &lt;br>
In Proceedings of the Conference on Computer-Supported Cooperative Work and Social Computing (CSCW'2018).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Online experimentation with volunteers relies on participants&amp;rsquo; non-financial motivations to complete a study,
such as to altruistically support science or to compare oneself to others. Researchers rely on these motivations
to attract study participants and often use incentives, like performance comparisons, to encourage participation.
Often, these study incentives are advertised using a slogan (e.g., &amp;ldquo;What is your thinking style?&amp;rdquo;). Research on
framing effects suggests that advertisement slogans attract people with varying demographics and motivations.
Could the slogan advertisements for studies risk attracting only specific users? To investigate the existence of
potential sample biases, we measured how different slogan frames affected which participants self-selected
into studies. We found that slogan frames impact recruitment significantly; changing the slogan frame from a
&amp;lsquo;supporting science&amp;rsquo; frame to a &amp;lsquo;comparing oneself to others&amp;rsquo; frame lead to a 9% increase in recruitment for
some studies. Additionally, slogans framed as learning more about oneself attract participants significantly
more motivated by boredom compared to other slogan frames. We discuss design implications for using frames
to improve recruitment and mitigate sources of sample bias in online research with volunteers.&lt;/p></description></item><item><title>human predictions</title><link>https://chenhaot.github.io/papers/human-predictions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/human-predictions/</guid><description>&lt;h2 id="on-human-predictions-with-explanations-and-predictions-of-machine-learning-models-a-case-study-on-deception-detection">On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection&lt;/h2>
&lt;p>&lt;a href="https://vivlai.github.io/">Vivian Lai&lt;/a> and Chenhao Tan. &lt;br>
In Proceedings of ACM FAT* Conference 2019.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are &lt;em>not&lt;/em> amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine
learning models affects human performance and human agency.&lt;/p></description></item><item><title>human-centered explanations workshop</title><link>https://chenhaot.github.io/papers/human-centered-explanations-workshop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/human-centered-explanations-workshop/</guid><description>&lt;h2 id="harnessing-explanations-to-bridge-ai-and-humans">Harnessing Explanations to Bridge AI and Humans&lt;/h2>
&lt;p>&lt;a href="https://vivlai.github.io/">Vivian Lai&lt;/a>, &lt;a href="http://scarton.people.si.umich.edu/">Samuel Carton&lt;/a>, and Chenhao Tan. &lt;br>
In Fair &amp;amp; Responsible AI Workshop, CHI 2020.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Machine learning models are increasingly integrated into societally critical applications such as recidivism prediction and medical diagnosis, thanks to their superior predictive power. In these applications, however, full automation is often not desired due to ethical and legal concerns. The research community has thus ventured into developing interpretable methods that explain machine predictions. While these explanations are meant to assist humans in understanding machine predictions and thereby allowing humans to make better decisions, this hypothesis is not supported in many recent studies. To improve human decision-making with AI assistance, we propose future directions for closing the gap between the efficacy of explanations and improvement in human performance.&lt;/p></description></item><item><title>human-centered tutorials</title><link>https://chenhaot.github.io/papers/human-centered-tutorials/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/human-centered-tutorials/</guid><description>&lt;h2 id="why-is-chicago-deceptive-towards-building-model-driven-tutorials-for-humans">&amp;ldquo;Why is &amp;lsquo;Chicago&amp;rsquo; deceptive?&amp;rdquo; Towards Building Model-Driven Tutorials for Humans&lt;/h2>
&lt;p>&lt;a href="https://vivlai.github.io/">Vivian Lai&lt;/a>, &lt;a href="https://mrsata.github.io/">Han Liu&lt;/a>, and Chenhao Tan. &lt;br>
In Proceedings of CHI 2020.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
To support human decision making with machine learning models, we often need to elucidate patterns embedded in the models that are unsalient, unknown, or counterintuitive to humans. While existing approaches focus on explaining machine predictions with real-time assistance, we explore model-driven tutorials to help humans understand these patterns in a training phase. We consider both tutorials with guidelines from scientific papers, analogous to current practices of science communication, and automatically selected examples from training data with explanations. We use deceptive review detection as a testbed and conduct large-scale, randomized human-subject experiments to examine the effectiveness of such tutorials. We find that tutorials indeed improve human performance, with and without real-time assistance. In particular, although deep learning provides superior predictive performance than simple models, tutorials and explanations from simple models are more useful to humans. Our work suggests future directions for human-centered tutorials and explanations towards a synergy between humans and AI.&lt;/p></description></item><item><title>idea relations</title><link>https://chenhaot.github.io/papers/idea-relations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/idea-relations/</guid><description>&lt;h2 id="friendships-rivalries-and-trysts-characterizing-relations-between-ideas-in-texts">Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts&lt;/h2>
&lt;p>Chenhao Tan, &lt;a href="http://www.cs.cmu.edu/~dcard/">Dallas Card&lt;/a>, &lt;a href="http://homes.cs.washington.edu/~nasmith/">Noah A. Smith&lt;/a> &lt;br>
In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL'2017)&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Understanding how ideas relate to each other is a fundamental question in many domains, ranging from intellectual history to public communication. Because ideas are naturally embedded in texts, we propose the first framework to systematically characterize the relations between ideas based on their occurrence in a corpus of documents, independent of how these ideas are represented. Combining two statistics &amp;mdash; &lt;em>cooccurrence within documents&lt;/em> and &lt;em>prevalence correlation over time&lt;/em> &amp;mdash; our approach reveals a number of different ways in which ideas can cooperate and compete. For instance, two ideas can closely track each other&amp;rsquo;s prevalence over time, and yet rarely cooccur, almost like a &amp;ldquo;cold war&amp;rdquo; scenario. We observe that pairwise cooccurrence and prevalence correlation exhibit different distributions. We further demonstrate that our approach is able to uncover intriguing relations between ideas through in-depth case studies on news articles and research papers.&lt;/p></description></item><item><title>Instant Foodie</title><link>https://chenhaot.github.io/papers/instant-foodie/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/instant-foodie/</guid><description>&lt;h2 id="instant-foodie-predicting-expert-ratings-from-grassroots">Instant Foodie: Predicting Expert Ratings From Grassroots&lt;/h2>
&lt;p>Chenhao Tan, &lt;a href="http://www-users.cs.umn.edu/~echi/">Ed H. Chi&lt;/a>, &lt;a href="http://www.davehuffaker.com">David Huffaker&lt;/a>, &lt;a href="https://sites.google.com/site/gkossinets/">Gueorgi Kossinets&lt;/a>, &lt;a href="alex.smola.org">Alexander J. Smola&lt;/a> &lt;br>
In Proceedings of 22nd ACM International Conference on Information and Knowledge Management (CIKM'2013)&lt;/p>
&lt;p>Consumer review sites and recommender systems typically rely on a large volume of user-contributed ratings, which makes rating acquisition an essential component in the design of such systems. User ratings are then summarized to provide an aggregate score representing a popular evaluation of an item. An inherent problem in such summarization is potential bias due to raters’ self-selection and heterogeneity in terms of experiences, tastes and rating scale interpretations. There are two major approaches to collecting ratings, which have different advantages and disadvantages. One is to allow a large number of volunteers to choose and rate items directly (a method employed by e.g. Yelp and Google Places). Alternatively, a panel of raters may be maintained and invited to rate a predefined set of items at regular intervals (such as in Zagat Survey). The latter approach arguably results in more consistent reviews and reduced selection bias, however, at the expense of much smaller coverage (fewer rated items).&lt;br>
In this paper, we examine the two different approaches to collecting user ratings of restaurants and explore the question of whether it is possible to reconcile them. Specifically, we study the problem of inferring the more calibrated Zagat Survey ratings (which we dub “expert ratings”) from the user-contributed ratings (“grassroots”) in Google Places. To achieve this, we employ latent factor models and provide a probabilistic treatment of the ordinal ratings. We can predict Zagat Survey ratings accurately from ad hoc usergenerated ratings by employing joint optimization. Furthermore, the resulting model show that users become more discerning as they submit more ratings. We also describe an approach towards cross-city recommendations, answering questions such as “What is the equivalent of the Per Se1 restaurant in Chicago?”&lt;/p></description></item><item><title>intergroup contact</title><link>https://chenhaot.github.io/papers/intergroup-contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/intergroup-contact/</guid><description>&lt;h2 id="intergroup-contact-in-the-wild-characterizing-language-differences-between-intergroup-and-single-group-members-in-nba-related-discussion-forums">Intergroup Contact in the Wild: Characterizing Language Differences between Intergroup and Single-group Members in NBA-related Discussion Forums&lt;/h2>
&lt;p>&lt;a href="http://www.jasondarkblue.com/">Jason Shuo Zhang&lt;/a>, &lt;em>Chenhao Tan&lt;/em>, and &lt;a href="https://www.cs.colorado.edu/~lv/">Qin Lv&lt;/a>. &lt;br>
In Proceedings of the Conference on Computer-Supported Cooperative Work and Social Computing (CSCW'2019).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Intergroup contact has long been considered as an effective strategy to reduce prejudice between groups. However, recent studies suggest that exposure to opposing groups in online platforms can exacerbate polarization. To further understand the behavior of individuals who actively engage in intergroup contact in practice, we provide a large-scale observational study of intragroup behavioral differences between members with and without intergroup contact. We leverage the existing structure of NBA-related discussion forums on Reddit to study the context of professional sports. We identify fans of each NBA team as members of a group and trace whether they have intergroup contact. Our results show that members with intergroup contact use more negative and abusive language in their affiliated group than those without such contact, after controlling for activity levels. We further quantify different levels of intergroup contact and show that there may exist nonlinear mechanisms regarding how intergroup contact relates to intragroup behavior. Our findings provide complementary evidence to experimental studies in a novel context, and also shed light on possible reasons for the different outcomes in prior studies.&lt;/p></description></item><item><title>learning from rationales</title><link>https://chenhaot.github.io/papers/learning-from-rationales/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/learning-from-rationales/</guid><description>&lt;h2 id="what-to-learn-and-how-toward-effective-learning-from-rationales">What to Learn, and How: Toward Effective Learning from Rationales&lt;/h2>
&lt;p>&lt;a href="https://shcarton.github.io/">Samuel Carton&lt;/a>, &lt;a href="https://surya-kanoria.github.io">Surya Kanoria&lt;/a>, and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
Findings of ACL 2022.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision. While intuitive, this idea has proven elusive in practice. We make two observations about human rationales via empirical analyses: 1) maximizing rationale supervision accuracy is not necessarily the optimal objective for improving model accuracy; 2) human rationales vary in whether they provide sufficient information for the model to exploit for prediction. Building on these insights, we propose several novel loss functions and learning strategies, and evaluate their effectiveness on three datasets with human rationales. Our results demonstrate consistent improvements over baselines in both label and rationale accuracy, including a 3% accuracy improvement on MultiRC. Our work highlights the importance of understanding properties of human explanations and exploiting them accordingly in model training.&lt;/p></description></item><item><title>lostinpropagation</title><link>https://chenhaot.github.io/papers/lostinpropagation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/lostinpropagation/</guid><description>Lost in Propagation? Unfolding News Cycles from the Source</description></item><item><title>migrant churn</title><link>https://chenhaot.github.io/papers/migrant-churn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/migrant-churn/</guid><description>&lt;h2 id="to-stay-or-to-leave-churn-prediction-for-urban-migrants-in-the-initial-period">To Stay or to Leave: Churn Prediction for Urban Migrants in the Initial Period&lt;/h2>
&lt;p>&lt;a href="http://yangy.org/">Yang Yang&lt;/a>, Zongtao Liu, Chenhao Tan, Fei Wu, Yueting Zhuang, and Yafeng Li. &lt;br>
In Proceedings of The Web Conference (WWW'2018).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
In China, 260 million people migrate to cities to realize their urban dreams. Despite that these migrants play an important role in the rapid urbanization process, many of them fail to settle down and eventually leave the city. The integration process of migrants thus raises an important issue for scholars and policymakers.&lt;/p></description></item><item><title>Migrant Integration</title><link>https://chenhaot.github.io/papers/migrant-integration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/migrant-integration/</guid><description>&lt;h4 id="urban-dreams-of-migrants-a-case-study-of-migrant-integration-in-shanghai">Urban Dreams of Migrants: A Case Study of Migrant Integration in Shanghai&lt;/h4>
&lt;p>&lt;a href="http://yangy.org/">Yang Yang&lt;/a>, Chenhao Tan, Zongtao Liu, Fei Wu, and Yueting Zhuang. &lt;br>
In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI'2018).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Unprecedented human mobility has driven the rapid urbanization around the world. In China, the fraction of population dwelling in cities increased from 17.9% to 52.6% between 1978 and 2012. Such large-scale migration poses challenges for policymakers and important questions for researchers.&lt;/p></description></item><item><title>Misc</title><link>https://chenhaot.github.io/misc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/misc/</guid><description>&lt;p>I attended CAOSS (Computational and Online Social Science) 2012 in NYC. I wrote a brief summary containing the works covered in the workshop. [&lt;a href="https://chenhaot.github.io/pubs/caoss.pdf">PDF&lt;/a>]&lt;/p>
&lt;p>I lived in &lt;a href="http://en.wikipedia.org/wiki/Telluride_Association">Telluride House&lt;/a> for four years. It has been quite an enriching experience to live with people from various backgrounds.&lt;/p>
&lt;p>My hometown is &lt;a href="http://en.wikipedia.org/wiki/Jingdezhen">Jingdezhen&lt;/a>. It is known as the &amp;ldquo;Porcelain Capital&amp;rdquo; for producing quality pottery for 1700 years.&lt;/p></description></item><item><title>Multi-community</title><link>https://chenhaot.github.io/papers/multi-community/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/multi-community/</guid><description>a longitudinal study of a user&amp;rsquo;s community trajectory</description></item><item><title>nation relations</title><link>https://chenhaot.github.io/papers/nation-relations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/nation-relations/</guid><description>&lt;h2 id="no-permanent-friends-or-enemies-tracking-relationships-between-nations-from-news">No Permanent Friends or Enemies: Tracking Relationships Between Nations from News&lt;/h2>
&lt;p>&lt;a href="https://xhan77.github.io/">Xiaochuang Han&lt;/a>, &lt;a href="http://homes.cs.washington.edu/~eunsol/home.html">Eunsol Choi&lt;/a>, and Chenhao Tan. &lt;br>
In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL'2019).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Understanding the dynamics of international politics is important yet challenging for civilians. In this work, we explore unsupervised neural models to infer relations between nations from news articles. We extend existing models by incorporating shallow linguistics information and propose a new automatic evaluation metric that aligns relationship dynamics with manually annotated key events. As understanding international relations requires carefully analyzing complex relationships, we conduct in-person human evaluations with three groups of participants. Overall, humans prefer the outputs of our model and give insightful feedback that suggests future directions for human-centered models. Furthermore, our model reveals interesting regional differences in news coverage. For instance, with respect to US-China relations, Singaporean media focus more on &amp;ldquo;strengthening&amp;rdquo; and &amp;ldquo;purchasing&amp;rdquo;, while US media focus more on &amp;ldquo;criticizing&amp;rdquo; and &amp;ldquo;denouncing&amp;rdquo;.&lt;/p></description></item><item><title>neural topic model metadata</title><link>https://chenhaot.github.io/papers/neural-topic-model-metadata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/neural-topic-model-metadata/</guid><description>&lt;h2 id="neural-models-for-documents-with-metadata">Neural Models for Documents with Metadata&lt;/h2>
&lt;p>&lt;a href="http://www.cs.cmu.edu/~dcard/">Dallas Card&lt;/a>, Chenhao Tan, and &lt;a href="http://homes.cs.washington.edu/~nasmith/">Noah A. Smith&lt;/a>. &lt;br>
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL'2018).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.&lt;/p></description></item><item><title>online nmf</title><link>https://chenhaot.github.io/papers/online-nmf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/online-nmf/</guid><description>&lt;h2 id="efficient-document-clustering-via-online-nonnegative-matrix-factorizations">Efficient Document Clustering via Online Nonnegative Matrix Factorizations&lt;/h2>
&lt;p>&lt;a href="http://sites.google.com/site/feiwang03/">Fei Wang&lt;/a>, Chenhao Tan, Arnd Christian Konig, &lt;a href="http://www.stat.cornell.edu/~li/">Ping Li&lt;/a> &lt;br>
In Proceedings of the SIAM International Conference on Data Mining (SDM'2011)&lt;/p>
&lt;p>In recent years, Nonnegative Matrix Factorization (NMF) has received considerable interest from the data mining and information retrieval fields. NMF has been successfully applied in document clustering, image representation, and other domains. This study proposes an online NMF (ONMF) algorithm to efficiently handle very large-scale and/or streaming datasets. Unlike conventional NMF solutions which require the entire data matrix to reside in the memory, our ONMF algorithm proceeds with one data point or one chunk of data points at a time. Experiments with one-pass and multi-pass ONMF on real datasets are presented.&lt;/p></description></item><item><title>online offline</title><link>https://chenhaot.github.io/papers/online-offline/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/online-offline/</guid><description>&lt;h2 id="this-is-why-we-play-characterizing-online-fan-communities-of-the-nba-teams">&amp;ldquo;This is why we play&amp;rdquo;: Characterizing Online Fan Communities of the NBA Teams&lt;/h2>
&lt;p>&lt;a href="http://www.jasondarkblue.com/">Jason Shuo Zhang&lt;/a>, &lt;em>Chenhao Tan&lt;/em>, and &lt;a href="https://www.cs.colorado.edu/~lv/">Qin Lv&lt;/a>. &lt;br>
In Proceedings of the Conference on Computer-Supported Cooperative Work and Social Computing (CSCW'2018).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Professional sports constitute an important part of people&amp;rsquo;s modern life. People spend substantial amounts of time and money supporting their favorite players and teams, and sometimes even riot after games. However, how team performance affects fan behavior remains understudied at a large scale. As almost every notable professional team has its own online fan community, these communities provide great opportunities for investigating this research question. In this work, we provide the first large-scale characterization of online fan communities of professional sports teams.&lt;/p></description></item><item><title>ood interactive explanation human ai decision making</title><link>https://chenhaot.github.io/papers/ood-interactive-explanation-human-ai-decision-making/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/ood-interactive-explanation-human-ai-decision-making/</guid><description>&lt;h2 id="understanding-the-effect-of-out-of-distribution-examples-and-interactive-explanations-on-human-ai-decision-making">Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making&lt;/h2>
&lt;p>&lt;a href="https://mrsata.github.io/">Han Liu&lt;/a>, &lt;a href="https://vivlai.github.io/">Vivian Lai&lt;/a>, and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Proceedings of CSCW 2021.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in-distribution because of distribution shift, we design experiments with different distribution types and investigate human performance for both in-distribution and out-of-distribution examples. Second, we develop novel interfaces to support interactive explanations so that humans can actively engage with AI assistance. Using virtual pilot studies and large-scale randomized experiments across three tasks, we demonstrate a clear difference between in-distribution and out-of-distribution, and observe mixed results for interactive explanations: while interactive explanations improve human perception of AI assistance&amp;rsquo;s usefulness, they may reinforce human biases and lead to limited performance improvement. Overall, our work points out critical challenges and future directions towards enhancing human performance with AI assistance.&lt;/p></description></item><item><title>pairreddit</title><link>https://chenhaot.github.io/papers/pairreddit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/pairreddit/</guid><description>&lt;h2 id="science-askscience-and-badscience-on-the-coexistence-of-highly-related-communities">Science, AskScience, and BadScience: On the Coexistence of Highly Related Communities&lt;/h2>
&lt;p>&lt;a href="http://www.cs.cornell.edu/~jhessel/">Jack Hessel&lt;/a>, Chenhao Tan, &lt;a href="http://www.cs.cornell.edu/home/llee">Lillian Lee&lt;/a>. &lt;br>
In Proceedings of the 10th International Conference on Web and Social Media (ICWSM'2016).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
When large social-media platforms allow users to easily form and self-organize into interest groups, highly related communities can arise. For example, the Reddit site hosts not just a group called &amp;ldquo;food&amp;rdquo;, but also &amp;ldquo;HealthyFood&amp;rdquo;, &amp;ldquo;foodhacks&amp;rdquo;, &amp;ldquo;foodporn&amp;rdquo;, and &amp;ldquo;cooking&amp;rdquo;, among others. Are these highly related communities created for similar classes of reasons (e.g. &amp;ldquo;true&amp;rdquo; to distinguish one as a better community and &amp;ldquo;advice&amp;rdquo; to focus on helping fellow members)? How do users allocate attention between such close alternatives when they are available or emerge over time? Are there different types of relations between close alternatives such as sharing many users vs. a new community drawing away members of an older one vs. a splinter group failing to cohere into a viable separate community? We investigate the interactions between highly related communities using data from reddit.com consisting of 975M posts and comments spanning an 8-year period. We identify a set of typical affixes that users adopt to create highly related communities and build a taxonomy of affixes. One interesting finding regarding users’ behavior is: after a newer community is created, for several types of highly-related community pairs, users that engage in a newer community tend to be more active in their original community than users that do not explore, even when controlling for previous level of engagement.&lt;/p></description></item><item><title>Papers</title><link>https://chenhaot.github.io/papers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/</guid><description>&lt;h2 id="a-full-list-of-my-publications-in-chronological-order">A full list of my publications in chronological order&lt;/h2>
&lt;p>[&lt;a href="http://scholar.google.com/citations?user=KGMaP18AAAAJ&amp;amp;hl=en">Google scholar profile&lt;/a>] (Google scholar is usually more up-to-date and I manually check all the paper entries on Google scholar.)&lt;/p>
&lt;p>Preprint&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://laoliu5280.github.io/">Haokun Liu*&lt;/a>, &lt;a href="https://rosafish.github.io">Yangqiaoyu Zhou*&lt;/a>, Mingxuan Li*, Chenfei Yuan, and &lt;em>Chenhao Tan&lt;/em>. &lt;a href="https://arxiv.org/abs/2410.17309">Literature Meets Data: A Synergistic Approach to Hypothesis Generation&lt;/a>. &lt;a href="https://chicagohai.github.io/hypogenic-demo/">[Website]&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://people.cs.uchicago.edu/~mourad/">Mourad Heddaya&lt;/a>, Kyle MacMillan, Anup Malani, Hongyuan Mei, and &lt;em>Chenhao Tan&lt;/em>. &lt;a href="http://arxiv.org/abs/2501.00097">CaseSumm: A Large-Scale Dataset for Long-Context Summarization from U.S. Supreme Court Opinions&lt;/a>&lt;/li>
&lt;li>Zhizhang Yuan, Fanqi Shen, Meng Li, Yuguo Yu, &lt;em>Chenhao Tan&lt;/em>, and &lt;a href="http://yangy.org/">Yang Yang&lt;/a>. &lt;a href="https://arxiv.org/abs/2402.10251">BrainWave: A Brain Signal Foundation Model for Clinical Applications&lt;/a>&lt;/li>
&lt;li>Yuyang Jiang, &lt;a href="https://chacha-chen.github.io">Chacha Chen&lt;/a>, &lt;a href="https://dangng2004.github.io/">Dang Nguyen&lt;/a>, Benjamin M. Mervak, and &lt;em>Chenhao Tan&lt;/em>. &lt;a href="https://arxiv.org/abs/2407.12176">GPT-4V Cannot Generate Radiology Reports Yet&lt;/a>.&lt;/li>
&lt;li>Jiashuo Wang, Yang Xiao, Yanran Li, Changhe Song, Chunpu Xu, &lt;em>Chenhao Tan&lt;/em>, and Wenjie Li. &lt;a href="https://arxiv.org/abs/2406.12266">Towards a Client-Centered Assessment of LLM Therapists by Client Simulation&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://chaochunhsu.github.io">Chao-Chun Hsu&lt;/a>, &lt;a href="http://ziadobermeyer.com/">Ziad Obermeyer&lt;/a>, and &lt;em>Chenhao Tan&lt;/em>. &lt;a href="https://arxiv.org/abs/2312.03077">Clinical Notes Reveal Physician Fatigue&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>2024&lt;/p></description></item><item><title>Personalization on text comprenhensibility</title><link>https://chenhaot.github.io/papers/personalization-on-text-comprenhensibility/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/personalization-on-text-comprenhensibility/</guid><description>&lt;h2 id="to-each-his-own-personalized-content-selection-based-on-text-comprehensibility">To Each His Own: Personalized Content Selection based on Text Comprehensibility&lt;/h2>
&lt;p>Chenhao Tan, &lt;a href="http://www.cs.technion.ac.il/~gabr/">Evgeniy Gabrilovich&lt;/a>, &lt;a href="https://sites.google.com/site/bopang42/">Bo Pang&lt;/a> &lt;br>
In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining (WSDM'2012)&lt;/p>
&lt;p>Imagine a physician and a patient doing a search on antibiotic resistance. Or a chess amateur and a grandmaster conducting a search on Alekhine’s Defence. Although the topic is the same, arguably the two users in each case will satisfy their information needs with very different texts. Yet today search engines mostly adopt the onesize-fits-all solution, where personalization is restricted to topical preference. We found that users do not uniformly prefer simple texts, and that the text comprehensibility level should match the user’s level of preparedness. Consequently, we propose to model the comprehensibility of texts as well as the users’ reading proficiency in order to better explain how different users choose content for further exploration. We also model topic-specific reading proficiency, which allows us to better explain why a physician might choose to read sophisticated medical articles yet simple descriptions of SLR cameras. We explore different ways to build user profiles, and use collaborative filtering techniques to overcome data sparsity. We conducted experiments on large-scale datasets from a major Web search engine and a community question answering forum. Our findings confirm that explicitly modeling text comprehensibility can significantly improve content ranking (search results or answers, respectively).&lt;/p></description></item><item><title>persuasive progress</title><link>https://chenhaot.github.io/papers/persuasive-progress/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/persuasive-progress/</guid><description>&lt;h2 id="measuring-online-debaters-persuasive-skill-from-text-over-time">Measuring Online Debaters&amp;rsquo; Persuasive Skill from Text over Time&lt;/h2>
&lt;p>Kelvin Luu, &lt;em>Chenhao Tan&lt;/em>, and &lt;a href="http://homes.cs.washington.edu/~nasmith/">Noah A. Smith&lt;/a> &lt;br>
Transactions of the Association for Computational Linguistics.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Online debates allow people to express their persuasive abilities and provide exciting opportunities for understanding persuasion. Prior studies have focused on studying persuasion in debate content, but without accounting for each debater’s history or exploring the progression of a debater’s persuasive ability. We study debater skill by modeling how participants progress over time in a collection of debates from Debate.org. We build on a widely used model of skill in two-player games and augment it with linguistic features of a debater’s content. We show that online debaters’ skill levels do tend to improve over time. Incorporating linguistic profiles leads to more robust skill estimation than winning records alone. Notably, we find that an interaction feature combining uncertainty cues (hedging) with terms strongly associated with either side of a particular debate (fightin’ words) is more predictive than either feature on its own, indicating the importance of fine- grained linguistic features.&lt;/p></description></item><item><title>polymath</title><link>https://chenhaot.github.io/papers/polymath/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/polymath/</guid><description>&lt;h2 id="internet-collaboration-on-extremely-difficult-problems-research-versus-olympiad-questions-on-the-polymath-site">Internet Collaboration on Extremely Difficult Problems: Research versus Olympiad Questions on the Polymath Site&lt;/h2>
&lt;p>Isabel Kloumann, Chenhao Tan, &lt;a href="http://www.cs.cornell.edu/home/kleinber">Jon Kleinberg&lt;/a>, &lt;a href="http://www.cs.cornell.edu/home/llee">Lillian Lee&lt;/a>. &lt;br>
In Proceedings of the 25th International World Wide Web Conference (WWW'2016).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Despite the existence of highly successful Internet collaborations on complex projects, including open-source software, little is known about how Internet collaborations work for solving “extremely” difficult problems, such as open-ended research questions. We quantitatively investigate a series of efforts known as the Polymath projects, which tackle mathematical research problems through open online discussion. A key analytical insight is that we can contrast the polymath projects with mini-polymaths — spinoffs that were conducted in the same manner as the polymaths but aimed at addressing math Olympiad questions, which, while quite difficult, are known to be feasible. Our comparative analysis shifts between three elements of the projects: the roles and relationships of the authors, the temporal dynamics of how the projects evolved, and the linguistic properties of the discussions themselves. We find interesting differences between the two domains through each of these analyses, and present these analyses as a template to facilitate comparison between Polymath and other domains for collaboration and communication. We also develop models that have strong performance in distinguishing research-level comments based on any of our groups of features. Finally, we examine whether comments representing research breakthroughs can be recognized more effectively based on their intrinsic features, or by the (re-)actions of others, and find good predictive power in linguistic features.&lt;/p></description></item><item><title>positive words negative reviews</title><link>https://chenhaot.github.io/papers/positive-words-negative-reviews/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/positive-words-negative-reviews/</guid><description>&lt;h2 id="on-positivity-bias-in-negative-reviews">On Positivity Bias in Negative Reviews&lt;/h2>
&lt;p>&lt;a href="https://madhu-aithal.github.io/">Madhusudhan Aithal Mahabhaleshwara&lt;/a> and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021) (short paper).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Prior work has revealed that positive words occur more frequently than negative words in human expressions, which is typically attributed to positivity bias, a tendency for people to report positive views of reality. But what about the language used in negative reviews? Consistent with prior work, we show that English negative reviews tend to contain more positive words than negative words, using a variety of datasets. We reconcile this observation with prior findings on the pragmatics of negation, and show that negations are commonly associated with positive words in negative reviews. Furthermore, in negative reviews, the majority of sentences with positive words express negative opinions based on sentiment classifiers, indicating some form of negation.&lt;/p></description></item><item><title>ratioanle annotation</title><link>https://chenhaot.github.io/papers/ratioanle-annotation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/ratioanle-annotation/</guid><description>&lt;h2 id="explaining-why-how-instructions-and-user-interfaces-impact-annotator-rationales-when-labeling-text-data">Explaining Why: How Instructions and User Interfaces Impact Annotator Rationales When Labeling Text Data&lt;/h2>
&lt;p>Jamar L. Sullivan, &lt;a href="https://wbrackenbury.github.io">Will Brackenbury&lt;/a>, &lt;a href="https://www.mcnutt.in">Andrew McNutt&lt;/a>, &lt;a href="https://cs.uchicago.edu/people/kevin-bryson/">Kevin Bryson&lt;/a>, Kwam Byll, &lt;a href="https://yuxinchen.org">Yuxin Chen&lt;/a>, &lt;a href="https://www.littmania.com">Michael Littman&lt;/a>, &lt;em>Chenhao Tan&lt;/em>, &lt;a href="http://blaseur.com">Blase Ur&lt;/a> &lt;br>
In Proceedings of NAACL 2022.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
In the context of data labeling, NLP researchers are increasingly interested in having humans select rationales, a subset of input tokens relevant to the chosen label. We conducted a 332-participant online user study to understand how humans select rationales, especially how different instructions and user interface affordances impact the rationales chosen. Participants labeled ten movie reviews as positive or negative, selecting words and phrases supporting their label as rationales. We varied the instructions given, the rationale-selection task, and the user interface. Partic- ipants often selected about 12% of input tokens as rationales, but selected fewer if unable to drag over multiple tokens at once. Whereas participants were near unanimous in their data labels, they were far less consistent in their rationales. The user interface affordances and task greatly impacted the types of rationales chosen. We also observed large variance across participants.&lt;/p></description></item><item><title>social action tracking</title><link>https://chenhaot.github.io/papers/social-action-tracking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/social-action-tracking/</guid><description>&lt;h2 id="social-action-tracking-via-noise-tolerant-time-varying-factor-graphs">Social Action Tracking via Noise Tolerant Time-varying Factor Graphs&lt;/h2>
&lt;p>Chenhao Tan, &lt;a href="http://keg.cs.tsinghua.edu.cn/persons/johan_ugander">Jie Tang&lt;/a>, &lt;a href="http://www.sunlab.org/">Jimeng Sun&lt;/a>, Quan Lin, Fengjiao Wang &lt;br>
In Proceedings of the Sixteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD'2010).&lt;/p>
&lt;p>Users’ behaviors (actions) in a social network are influenced by various factors such as personal interests, social influence, and global trends. However, few publications systematically study how social actions evolve in a dynamic social network and to what extent different factors affect the user actions.
In this paper, we propose a Noise Tolerant Time-varying Factor Graph Model (NTT-FGM) for modeling and predicting social actions. NTT-FGM simultaneously models social network structure, user attributes and user action history for better prediction of the users’ future actions. More specifically, a user’s action at time t is generated by her latent state at t, which is influenced by her attributes, her own latent state at time t-1 and her neighbors’ states at time t and t-1. Based on this intuition, we formalize the social action tracking problem using the NTT-FGM model; then present an efficient algorithm to learn the model, by combining the ideas from both continuous linear system and Markov random field.
Finally, we present a case study of our model on predicting future social actions. We validate the model on three different types of real-world data sets. Qualitatively, our model can discover interesting patterns of the social dynamics. Quantitatively, experimental results show that the proposed method outperforms several baseline methods for social action prediction.&lt;/p></description></item><item><title>Social topical structure</title><link>https://chenhaot.github.io/papers/social-topical-structure/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/social-topical-structure/</guid><description>&lt;h2 id="on-the-interplay-between-social-and-topical-structure">On the Interplay between Social and Topical Structure&lt;/h2>
&lt;p>&lt;a href="http://www.dromero.org/">Daniel M. Romero&lt;/a>, Chenhao Tan, &lt;a href="http://people.cam.cornell.edu/~jugander/">Johan Ugander&lt;/a> &lt;br>
In Proceedings of 7th International AAAI Conference on Weblogs and Social Media (ICWSM'2013)&lt;/p>
&lt;p>People’s interests and people’s social relationships are intuitively connected, but understanding their interplay and whether they can help predict each other has remained an open question. We examine the interface of two decisive structures forming the backbone of online social media: the graph structure of social networks &amp;mdash; who connects with whom &amp;mdash; and the set structure of topical affiliations — who is interested in what. In studying this interface, we identify key relationships whereby each of these structures can be understood in terms of the other. The context for our analysis is Twitter, a complex social network of both follower relationships and communication relationships. On Twitter, “hashtags” are used to label conversation topics, and we examine hashtag usage alongside these social structures. &lt;br>
We find that the hashtags that users adopt can predict their social relationships, and also that the social relationships between the initial adopters of a hashtag can predict the future popularity of that hashtag. By studying weighted social relationships, we observe that while strong reciprocated ties are the easiest to predict from hashtag structure, they are also much less useful than weak directed ties for predicting hashtag popularity. Importantly, we show that computationally simple structural determinants can provide remarkable performance in both tasks. While our analyses focus on Twitter, we view our findings as broadly applicable to topical affiliations and social relationships in a host of diverse contexts, including the movies people watch, the brands people like, or the locations people frequent.&lt;/p></description></item><item><title>Statement Strength</title><link>https://chenhaot.github.io/papers/statement-strength/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/statement-strength/</guid><description>&lt;h2 id="a-corpus-of-sentence-level-revisions-in-academic-writing-a-step-towards-understanding-statement-strength-in-communication">A Corpus of Sentence-level Revisions in Academic Writing: A Step towards Understanding Statement Strength in Communication&lt;/h2>
&lt;p>Chenhao Tan, &lt;a href="http://www.cs.cornell.edu/home/llee">Lillian Lee&lt;/a> &lt;br>
In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL'2014) (short paper)&lt;/p>
&lt;p>The strength with which a statement is made can have a significant impact on the audience. For example, international relations can be strained by how the media in one country describes an event in another; and papers can be rejected because they overstate or understate their findings. It is thus important to understand the effects of statement strength. A first step is to be able to distinguish between strong and weak statements. However, even this problem is understudied, partly due to a lack of data. Since strength is inherently relative, {em revisions} of texts that make claims are a natural source of data on strength differences. In this paper, we introduce a corpus of sentence-level revisions from academic writing. We also describe insights gained from our annotation efforts for this task.&lt;/p></description></item><item><title>Students</title><link>https://chenhaot.github.io/students/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/students/</guid><description>&lt;p>Check our lab &lt;a href="https://chicagohai.github.io/">page&lt;/a> for more updated information!&lt;/p>
&lt;p>Postdocs&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://shcarton.github.io/">Sam Carton&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.shifeng.umiacs.io">Shi Feng&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>PhDs&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://www.jasondarkblue.com/">Jason Shuo Zhang&lt;/a> (co-advised with Qin Lv, Outstanding PhD Research Award in 2019 &amp;mdash;&amp;gt; postdoc at Max Planck Institute)&lt;/li>
&lt;li>&lt;a href="https://vivlai.github.io/">Vivian Lai&lt;/a> (First place at CS Research Expo 2019)&lt;/li>
&lt;li>&lt;a href="https://joe32140.github.io/">Chao-Chun Hsu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mrsata.github.io/">Han Liu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://rosafish.github.io/">Yangqiaoyu (Rosa) Zhou&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://chacha-chen.github.io">Chacha Chen&lt;/a>&lt;/li>
&lt;li>Mourad Heddaya&lt;/li>
&lt;li>&lt;a href="https://karen-zhou.com">Karen Zhou&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Masters&lt;/p>
&lt;ul>
&lt;li>Yiming Zhang (class of 2023, University of Chicago)&lt;/li>
&lt;li>Harry Tian (class of 2023, University of Chicago)&lt;/li>
&lt;li>Rajat Bhatnagar (class of 2021, University of Colorado Boulder)&lt;/li>
&lt;li>Madhusudhan Aithal Mahabhaleshwara (class of 2021, University of Colorado Boulder)&lt;/li>
&lt;li>Anirudh Rathore (class of 2021, University of Colorado Boulder)&lt;/li>
&lt;li>Brian Lubars (class of 2019, University of Colorado Boulder)&lt;/li>
&lt;li>Shantanu Karnwal (class of 2019, University of Colorado Boulder)&lt;/li>
&lt;li>Kumar Bhargav Srinivasan (class of 2019, University of Colorado Boulder)&lt;/li>
&lt;li>Hunter Wapman (class of 2019, University of Colorado Boulder)&lt;/li>
&lt;li>Vivian Lai (class of 2018, University of Colorado Boulder)&lt;/li>
&lt;/ul>
&lt;p>Undergraduates&lt;/p></description></item><item><title>Talks</title><link>https://chenhaot.github.io/talks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/talks/</guid><description>&lt;h2 id="slides-for-humanai-conference-in-2022">Slides for Human+AI conference in 2022&lt;/h2>
&lt;p>Jenn Wortman Vaughan, &lt;a href="https://drive.google.com/file/d/1TRQz3s3wYwDKGFrwvnWDje0azkayCzfv/view">Some Very Human Challenges in Responsible AI&lt;/a>&lt;/p>
&lt;p>Berkeley Dietvorst, &lt;a href="https://drive.google.com/file/d/1Q5ichZSqMXZr0M0rmtonzwapVYwK_9VV/view">Aligning algorithms with people&amp;rsquo;s prediction preferences&lt;/a>&lt;/p>
&lt;p>Krzysztof Gajos, &lt;a href="https://www.dropbox.com/s/ht2cmjulebk9lev/2022.10.28%20-%20Chicago%20-%20Human%20Cognitive%20(Dis)Engagement%20during%20AI-Assisted%20Decision-Making.pdf?dl=0">Human Cognitive (Dis)Engagement During AI-Assisted Decision Making&lt;/a>&lt;/p>
&lt;p>Danny Oppenheimer, &lt;a href="https://drive.google.com/file/d/1a7BJsfx4_887L9wUQ_pbStMDeu--8bCV/view">Decision Science the Age of Augmented Cognition&lt;/a>&lt;/p>
&lt;p>Marc Berman, &lt;a href="https://drive.google.com/file/d/1bVZqk274vF3JzIaqu6ceTs7-bXVjD4F3/view">Using Theory, Sensors, and Machines to Quantify
the Nature of Social Interactions&lt;/a>&lt;/p>
&lt;h2 id="invited-talks">Invited talks&lt;/h2>
&lt;p>Towards Human-Centered AI: Understanding and Improving Human Decision Making through Explanations.&lt;/p>
&lt;ul>
&lt;li>Columbia University, 2023.&lt;/li>
&lt;li>Georgia Tech, 2023.&lt;/li>
&lt;li>MIT, 2023.&lt;/li>
&lt;li>University of Washington, 2023.&lt;/li>
&lt;li>Machine Learning in Economics Summer Institute, 2023.&lt;/li>
&lt;/ul>
&lt;p>Three Lessons Towards Human-Centered Explanations. Keynote at the Artificial Intelligence &amp;amp; Human Computer Interaction workshop at ICML 2023.&lt;/p></description></item><item><title>team composition</title><link>https://chenhaot.github.io/papers/team-composition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/team-composition/</guid><description>&lt;h2 id="what-makes-a-good-team-a-large-scale-study-on-the-effect-of-team-composition-in-online-gaming">What Makes a Good Team? A Large-scale Study on the Effect of Team Composition in Online Gaming&lt;/h2>
&lt;p>Ziqiang Cheng, &lt;a href="http://yangy.org/">Yang Yang&lt;/a>, &lt;em>Chenhao Tan&lt;/em>, Denny Cheng, Alex Cheng and Yueting Zhuang. &lt;br>
In Proceedings of The Web Conference (WWW'2019).&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Team composition is a central factor in determining the effectiveness of a team. In this paper, we present a large-scale study on the effect of team composition on multiple measures of team effectiveness. We use a dataset from the largest multiplayer online battle arena (MOBA) game, Honor of Kings, with 96 million matches involving 100 million players. We measure team effectiveness based on team performance (whether a team is going to win), team tenacity (whether a team is going to surrender), and team rapport (whether a team uses abusive language). Our results confirm the importance of team diversity with respect to player roles, and show that diversity has varying effects on team effectiveness: although diverse teams perform well and show tenacity in adversity, they are more likely to abuse when losing than less diverse teams. Our study also contributes to the situation vs. personality debate and show that abusive players tend to choose the leading role and players do not become more abusive when taking such roles.&lt;/p></description></item><item><title>unifying explanations</title><link>https://chenhaot.github.io/papers/unifying-explanations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/unifying-explanations/</guid><description>&lt;h2 id="towards-unifying-feature-attribution-and-counterfactual-explanations-different-means-to-the-same-end">Towards Unifying Feature Attribution and Counterfactual Explanations: Different Means to the Same End&lt;/h2>
&lt;p>&lt;a href="https://raam93.github.io/">Ramaravind Kommiya Mothilal&lt;/a>, &lt;a href="http://divy.at/">Divyat Mahajan&lt;/a>, &lt;em>Chenhao Tan&lt;/em> and &lt;a href="amit_sharma">Amit Sharma&lt;/a>. &lt;br>
In Proceedings of the Fourth AAAI/ACM conference on Artificial Intelligence, Ethics, and Society (AIES'2021); also presented at the &lt;a href="https://sites.google.com/view/rai-workshop/home?authuser=0">Reponsible AI workshop&lt;/a> at ICLR.&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Feature attributions and counterfactual explanations are popular approaches to explain a ML model. The former assigns an impor- tance score to each input feature, while the latter provides input examples with minimal changes to alter the model’s predictions. To unify these approaches, we provide an interpretation based on the actual causality framework and present two key results in terms of their use. First, we present a method to generate feature attribution explanations from a set of counterfactual examples. These feature attributions convey how important a feature is to changing the classification outcome of a model, especially on whether a subset of features is necessary and/or sufficient for that change, which attribution-based methods are unable to provide. Second, we show how counterfactual examples can be used to evaluate the goodness of an attribution-based explanation in terms of its necessity and sufficiency. As a result, we highlight the complementarity of these two approaches. Our evaluation on three benchmark datasets — Adult-Income, LendingClub, and German-Credit — confirms the complementarity. Feature attribution methods like LIME and SHAP and counterfactual explanation methods like Wachter et al. and DiCE often do not agree on feature importance rankings. In addi- tion, by restricting the features that can be modified for generating counterfactual examples, we find that the top-k features from LIME or SHAP are often neither necessary nor sufficient explanations of a model’s prediction. Finally, we present a case study of different explanation methods on a real-world hospital triage problem.&lt;/p></description></item><item><title>User-level sentiment</title><link>https://chenhaot.github.io/papers/user-level-sentiment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/user-level-sentiment/</guid><description>&lt;h2 id="user-level-sentiment-analysis-incorporating-social-networks">User-Level Sentiment Analysis Incorporating Social Networks&lt;/h2>
&lt;p>Chenhao Tan, &lt;a href="http://www.cs.cornell.edu/home/llee">Lillian Lee&lt;/a>, &lt;a href="http://keg.cs.tsinghua.edu.cn/persons/johan_ugander">Jie Tang&lt;/a>, Long Jiang, &lt;a href="http://research.microsoft.com/en-us/people/mingzhou">Ming Zhou&lt;/a>, &lt;a href="http://www.stat.cornell.edu/~li/">Ping Li&lt;/a> &lt;br>
In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD'2011) (poster, aggregate oral+poster acceptance rate: 17.5%)&lt;/p>
&lt;p>We show that information about social relationships can be used to
improve user-level sentiment analysis. The main motivation behind
our approach is that users that are somehow &amp;ldquo;connected&amp;rdquo; may be
more likely to hold similar opinions; therefore, relationship information can complement what we can extract about a user’s viewpoints from their utterances. Employing Twitter as a source for our
experimental data, and working within a semi-supervised framework, we propose models that are induced either from the Twitter
follower/followee network or from the network in Twitter formed
by users referring to each other using “@” mentions. Our transductive learning results reveal that incorporating social-network
information can indeed lead to statistically signiﬁcant sentimentclassiﬁcation improvements over the performance of an approach
based on Support Vector Machines having access only to textual
features.&lt;/p></description></item><item><title>value of information</title><link>https://chenhaot.github.io/papers/value-of-information/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/value-of-information/</guid><description>&lt;h2 id="characterizing-the-value-of-information-in-medical-notes">Characterizing the Value of Information in Medical Notes&lt;/h2>
&lt;p>&lt;a href="https://joe32140.github.io/">Chao-Chun Hsu&lt;/a>, Shantanu Karnwal, &lt;a href="sendhil_mullainathan">Sendhil Mullainathan&lt;/a>, &lt;a href="http://ziadobermeyer.com/">Ziad Obermeyer&lt;/a>, and &lt;em>Chenhao Tan&lt;/em>. &lt;br>
In Findings of EMNLP 2020&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong>&lt;br>
Machine learning models depend on the quality of input data. As electronic health records are widely adopted, the amount of data in health care is growing, along with complaints about the quality of medical notes. We use two prediction tasks, readmission prediction and in-hospital mortality prediction, to characterize the value of information in medical notes. We show that as a whole, medical notes only provide additional predictive power over structured information in readmission prediction. We further propose a probing framework to select parts of notes that enable more accurate predictions than using all notes, despite that the selected information leads to a distribution shift from the training data (&amp;ldquo;all notes&amp;rdquo;). Finally, we demonstrate that models trained on the selected valuable information achieve even better predictive performance, with only 6.8% of all the tokens for readmission prediction.&lt;/p></description></item><item><title>Wording for propagation</title><link>https://chenhaot.github.io/papers/wording-for-propagation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://chenhaot.github.io/papers/wording-for-propagation/</guid><description>&lt;h2 id="the-effect-of-wording-on-message-propagation-topic--and-author-controlled-natural-experiments-on-twitter">The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter&lt;/h2>
&lt;p>Chenhao Tan, &lt;a href="http://www.cs.cornell.edu/home/llee">Lillian Lee&lt;/a>, &lt;a href="https://sites.google.com/site/bopang42/">Bo Pang&lt;/a> &lt;br>
In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL'2014)&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> &lt;br>
Consider a person trying to spread an important message on a social network. He/she can spend hours trying to craft the message.
Does it actually matter? While there has been extensive prior work looking into predicting popularity of social-media content,
the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic.
To control for these confounding factors, we take advantage of the surprising fact that there are many
pairs of tweets containing the
&lt;em>same&lt;/em> url and written by the &lt;em>same&lt;/em> user but employing different
wording.
Given such pairs, we ask:
which version attracts more retweets?
This turns out to be a more difficult task than predicting popular topics.
Still, humans can answer this question better than chance (but far from
perfectly), and the computational methods we
develop
can do better than an average human as well as a strong competing method
trained on non-controlled data.&lt;/p></description></item></channel></rss>