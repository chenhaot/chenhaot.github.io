<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Biography | A minimal Hugo website</title>
<link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><script src=https://kit.fontawesome.com/cdd93c4598.js crossorigin=anonymous></script></head><body><nav><ul class=menu><li><a href=/>Home</a></li><li><a href=/papers/>Papers</a></li><li><a href=/talks/>Talks</a></li><li><a href=/courses/>Courses</a></li><li><a href=/contact/>Contact</a></li><li><a href=/blog/>Blog</a></li><li><a href=https://chicagohai.github.io/>Lab</a></li></ul><hr></nav><main><h2 id=biography>Biography</h2><p>I moved to the University of Chicago after 3.5 wonderful years at Boulder. Before Boulder, I was a postdoctoral researcher at <a href=http://www.washington.edu/>University of Washington</a>, working with <a href=http://homes.cs.washington.edu/~nasmith/>Noah Smith</a> for a year.
Prior to that, I spent wonderful years at Ithaca working with <a href=http://www.cs.cornell.edu/home/llee>Lillian Lee</a> and <a href=/collaborators.html>many others</a>,
and obtained a PhD degree in <a href=http://www.cs.cornell.edu/>Computer Science</a> at <a href=http://www.cornell.edu/>Cornell University</a>.
Originally, I am from <a href=http://en.wikipedia.org/wiki/Jingdezhen>Jingdezhen, China</a> and lived in Beijing for four years while studying <a href=http://www.tsinghua.edu.cn/publish/csen/>Computer Science</a> and <a href=http://www.sem.tsinghua.edu.cn/portalweb/appmanager/portal/semEN>Economics</a> at <a href=http://www.tsinghua.edu.cn/publish/then/index.html>Tsinghua University</a>.</p><p>You are here probably because you are interested in learning about life trajectories. I have a <a href=/papers/multi-community>paper</a> on that subject, and the main takeaway is &ldquo;People, unlike trees, thrive by relocation&rdquo;.</p><p>If you are looking for a formal bio, here is the most recent one:</p><p>Chenhao Tan is an associate professor of computer science and data science at the University of Chicago, and is also a visiting scientist at Abridge. He obtained his PhD degree in the Department of Computer Science at Cornell University and bachelor&rsquo;s degrees in computer science and in economics from Tsinghua University. Prior to joining the University of Chicago, he was an assistant professor at the University of Colorado Boulder and a postdoc at the University of Washington. His research interests include human-centered AI, natural language processing, and computational social science. His work has been covered by many news media outlets, such as the New York Times and the Washington Post. He also won a Sloan research fellowship, an NSF CAREER award, an NSF CRII award, a Google research scholar award, research awards from Amazon, IBM, JP Morgan, and Salesforce, a Facebook fellowship, and a Yahoo! Key Scientific Challenges award.</p><hr><h2 id=history-of-research-description-on-the-homepage>History of research description on the homepage:</h2><h4 id=01012025->01/01/2025 &mdash;</h4><p>My research brings together social sciences and machine learning to develop the best AI for humans. The applications that I am most interested in are 1) scientific discoveries, 2) healthcare, and 3) governance and democratic processes (ordered by random coin flips).</p><p>The central question of my current interest lies in how we can build an effective communication protocol between humans and AI. Here are some example problems that I think will contribute to this question:</p><ul><li>Specification: How can we help AI understand human goals and help humans specify their goals? Prompting and reinforcement learning from human feedback are the main paradigms right now, but what about goals that cannot be easily described or elicited through human preferences? Examples include <a href=https://chicagohai.github.io/hypogenic-demo/>data-driven hypothesis generation</a> and <a href=https://arxiv.org/abs/2109.06896>decision-focused summarization</a>.</li><li>Complementary AI: How can we develop AI that accounts for human intuitions/biases and complement human intelligence? This requires us to model human intuitions/biases and address them constructively. We have done some prior work on the important role of human similarity judgments in <a href=https://arxiv.org/abs/2303.04809>case-based explanations</a> and generally <a href=https://arxiv.org/abs/2202.04092>machine explanations</a>. But we will need fundamental breakthroughs in behavioral sciences, which in turn inform AI modeling/inference.</li><li>Interpretability: How can we make sense of (powerful) AI that is much better than humans? I am most interested in interpretability for expanding human knowledge (e.g., in scientific discoveries) and for improving the controllability of AI. It is important to explore opportunities in the absence of identifiability.</li></ul><p>In general, goals matter more to me than the specific problem or method. For instance, while hallucination is not among the above problems, I spend a lot of time thinking about hallucination at Abridge, as non-factual statements are very problematic for generating clinical notes in healthcare.</p><h4 id=unknown-beginning-of-time--12312024>Unknown beginning of time &mdash; 12/31/2024</h4><p>My research brings together social sciences and machine learning to develop the best AI for humans. Specifically, my work aims to enable effective human-AI interaction by</p><ol><li><p>Understanding human decision making through language. We analyze large amounts of textual data to unfold the connection between language and decisions in two directions. In one direction, we leverage natural experiments to understand how language shapes human decisions (e.g., <a href=/papers/changemyview.html>what makes effective persuasion</a> and <a href=https://arxiv.org/abs/2306.07117>language of bargaining</a>). In the other direction, we examine explanations of human decisions to identify their biases.</p></li><li><p>Generating human-centered explanations. <a href=https://chenhaot.com/papers/human-predictions.html>Our work</a> shows that current methods of generating explanations for AI predictions fail to improve human-AI decision making. We develop <a href=https://arxiv.org/abs/2202.04092>a novel theoretical framework</a> to show that the missing link is to model human interpretation of AI explanations. We thus build algorithms to <a href=https://arxiv.org/abs/2303.04809>align AI explanations with human intuitions</a> and demonstrate substantial improvements in human performance.</p></li><li><p>Developing novel paradigms of human-AI interaction. We explore additional possibilities that humans and AI can complement each other in three directions: 1) appropriate and effective <a href=https://chenhaot.com/papers/conditional-delegation.html>delegation</a> to AI; 2) <a href=https://chenhaot.com/papers/decision-focused-summarization.html>decision-focused summarization</a>, a novel formulation of the classic NLP task to identify the most relevant information to support decision making; and 3) <a href=https://arxiv.org/abs/2306.08042>few-shot learning from human explanations</a> so that humans can effectively improve large language models (LLMs).</p></li></ol></main><footer><hr>Â© Chenhao Tan 2025 (made with <a href=https://github.com/yihui/hugo-xmin/>Hugo XMin</a>) | <a href=https://github.com/ChicagoHAI/>Github</a> | <a href="https://scholar.google.com/citations?user=KGMaP18AAAAJ&amp;hl=en">Google Scholar</a> | <a href=https://bsky.app/profile/chenhaotan.bsky.social>Bluesky</a> | <a href=https://www.linkedin.com/in/chenhao-tan-2a446316/>LinkedIn</a></footer></body></html>