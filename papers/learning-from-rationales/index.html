<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>learning from rationales | A minimal Hugo website</title>
<link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><script src=https://kit.fontawesome.com/cdd93c4598.js crossorigin=anonymous></script></head><body><nav><ul class=menu><li><a href=/>Home</a></li><li><a href=/papers/>Papers</a></li><li><a href=/talks/>Talks</a></li><li><a href=/courses/>Courses</a></li><li><a href=/contact/>Contact</a></li><li><a href=/blog/>Blog</a></li><li><a href=https://chicagohai.github.io/>Lab</a></li></ul><hr></nav><main><h2 id=what-to-learn-and-how-toward-effective-learning-from-rationales>What to Learn, and How: Toward Effective Learning from Rationales</h2><p><a href=https://shcarton.github.io/>Samuel Carton</a>, <a href=https://surya-kanoria.github.io>Surya Kanoria</a>, and <em>Chenhao Tan</em>.<br>Findings of ACL 2022.</p><p><strong>Abstract:</strong><br>Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision. While intuitive, this idea has proven elusive in practice. We make two observations about human rationales via empirical analyses: 1) maximizing rationale supervision accuracy is not necessarily the optimal objective for improving model accuracy; 2) human rationales vary in whether they provide sufficient information for the model to exploit for prediction. Building on these insights, we propose several novel loss functions and learning strategies, and evaluate their effectiveness on three datasets with human rationales. Our results demonstrate consistent improvements over baselines in both label and rationale accuracy, including a 3% accuracy improvement on MultiRC. Our work highlights the importance of understanding properties of human explanations and exploiting them accordingly in model training.</p><p>[<a href=https://arxiv.org/pdf/2112.00071.pdf>PDF</a>]
[<a href=https://github.com/ChicagoHAI/learning-from-rationales>Code</a>]</p><p><img src=https://chenhaot.com/pubs/acl22.png alt="recall matters more than precision."></p><p>@inproceedings{carton+kanoria+tan:21,<br>    
author = {Samuel Carton and Surya Kanoria and Chenhao Tan},<br>    
title = {What to Learn, and How: Toward Effective Learning from Rationales},<br>    
year = {2022},<br>    
booktitle = {Findings of ACL}<br>}</p></main><footer><hr>© Chenhao Tan 2025 (made with <a href=https://github.com/yihui/hugo-xmin/>Hugo XMin</a>) | <a href=https://github.com/ChicagoHAI/>Github</a> | <a href="https://scholar.google.com/citations?user=KGMaP18AAAAJ&amp;hl=en">Google Scholar</a> | <a href=https://bsky.app/profile/chenhaotan.bsky.social>Bluesky</a> | <a href=https://www.linkedin.com/in/chenhao-tan-2a446316/>LinkedIn</a></footer></body></html>