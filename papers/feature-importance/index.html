<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>feature importance | Welcome to Chenhao Tan's Personal Website!</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><script src=https://kit.fontawesome.com/cdd93c4598.js crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-QD5VPF7ZFK"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QD5VPF7ZFK")}</script><meta property="og:type" content="website"><meta property="og:url" content="https://chenhaot.github.io/papers/feature-importance/"><meta property="og:title" content="feature importance | Welcome to Chenhao Tan's Personal Website!"><meta property="og:description" content="Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification
Vivian Lai, Jon Z. Cai, and Chenhao Tan.       
In Proceedings of 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP'2019)
Abstract:
Feature importance is commonly used to explain machine predictions. While feature importance can be derived from a machine learning model with a variety of methods, the consistency of feature importance via different methods remains understudied. In this work, we systematically compare feature importance from built-in mechanisms in a model such as attention values and post-hoc methods that approximate model behavior such as LIME. Using text classification as a testbed, we find that 1) no matter which method we use, important features from traditional models such as SVM and XGBoost are more similar with each other, than with deep learning models; 2) post-hoc methods tend to generate more similar important features for two models than built-in methods. We further demonstrate how such similarity varies across instances. Notably, important features do not always resemble each other better when two models agree on the predicted label than when they disagree."><meta property="og:image" content="https://chenhaot.github.io//images/web_circle_small.jpg"><meta property="twitter:card" content="summary"><meta property="twitter:url" content="https://chenhaot.github.io/papers/feature-importance/"><meta property="twitter:title" content="feature importance | Welcome to Chenhao Tan's Personal Website!"><meta property="twitter:description" content="Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification
Vivian Lai, Jon Z. Cai, and Chenhao Tan.       
In Proceedings of 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP'2019)
Abstract:
Feature importance is commonly used to explain machine predictions. While feature importance can be derived from a machine learning model with a variety of methods, the consistency of feature importance via different methods remains understudied. In this work, we systematically compare feature importance from built-in mechanisms in a model such as attention values and post-hoc methods that approximate model behavior such as LIME. Using text classification as a testbed, we find that 1) no matter which method we use, important features from traditional models such as SVM and XGBoost are more similar with each other, than with deep learning models; 2) post-hoc methods tend to generate more similar important features for two models than built-in methods. We further demonstrate how such similarity varies across instances. Notably, important features do not always resemble each other better when two models agree on the predicted label than when they disagree."><meta property="twitter:image" content="https://chenhaot.github.io//images/web_circle_small.jpg"></head><body><nav><ul class=menu><li><a href=/>Home</a></li><li><a href=/papers/>Papers</a></li><li><a href=/talks/>Talks</a></li><li><a href=/courses/>Courses</a></li><li><a href=/contact/>Contact</a></li><li><a href=https://substack.com/@cichicago>Blog</a></li><li><a href=https://chicagohai.github.io/>Lab</a></li></ul><hr></nav><main><h2 id=many-faces-of-feature-importance-comparing-built-in-and-post-hoc-feature-importance-in-text-classification>Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification</h2><p><a href=https://vivlai.github.io/>Vivian Lai</a>, <a href=https://joncaizheng.com/>Jon Z. Cai</a>, and <em>Chenhao Tan</em>.<br>In Proceedings of 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP'2019)</p><p><strong>Abstract:</strong><br>Feature importance is commonly used to explain machine predictions. While feature importance can be derived from a machine learning model with a variety of methods, the consistency of feature importance via different methods remains understudied. In this work, we systematically compare feature importance from built-in mechanisms in a model such as attention values and post-hoc methods that approximate model behavior such as LIME. Using text classification as a testbed, we find that 1) no matter which method we use, important features from traditional models such as SVM and XGBoost are more similar with each other, than with deep learning models; 2) post-hoc methods tend to generate more similar important features for two models than built-in methods. We further demonstrate how such similarity varies across instances. Notably, important features do not always resemble each other better when two models agree on the predicted label than when they disagree.</p><p>[<a href=https://chenhaot.com/pubs/explanations/feature-importance.pdf>PDF</a>]
[<a href=https://github.com/BoulderDS/feature-importance>Code</a>]</p><p><img src=https://chenhaot.com/pubs/explanations/feature.png alt=poster.></p><p>@inproceedings{lai+cai+tan:19,<br>    
author = {Vivian Lai and Jon Z. Cai and Chenhao Tan},<br>    
title = {Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification},<br>    
year = {2019},<br>    
booktitle = {Proceedings of EMNLP}<br>}</p></main><footer><hr><p>© Chenhao Tan 2026 (made with <a href=https://github.com/yihui/hugo-xmin/>Hugo XMin</a>)</p><p><a href=https://github.com/ChicagoHAI/>Github</a> | <a href="https://scholar.google.com/citations?user=KGMaP18AAAAJ&amp;hl=en">Google Scholar</a> | <a href=https://x.com/ChenhaoTan>X</a> | <a href=https://bsky.app/profile/chenhaotan.bsky.social>Bluesky</a> | <a href=https://www.linkedin.com/in/chenhao-tan-2a446316/>LinkedIn</a></p></footer></body></html>