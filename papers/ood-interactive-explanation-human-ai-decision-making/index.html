<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ood interactive explanation human ai decision making | Welcome to Chenhao Tan's Personal Website!</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><script src=https://kit.fontawesome.com/cdd93c4598.js crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-QD5VPF7ZFK"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QD5VPF7ZFK")}</script><meta property="og:type" content="website"><meta property="og:url" content="https://chenhaot.github.io/papers/ood-interactive-explanation-human-ai-decision-making/"><meta property="og:title" content="ood interactive explanation human ai decision making | Welcome to Chenhao Tan's Personal Website!"><meta property="og:description" content="Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making
Han Liu, Vivian Lai, and Chenhao Tan.       
In Proceedings of CSCW 2021.
Abstract:
Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in-distribution because of distribution shift, we design experiments with different distribution types and investigate human performance for both in-distribution and out-of-distribution examples. Second, we develop novel interfaces to support interactive explanations so that humans can actively engage with AI assistance. Using virtual pilot studies and large-scale randomized experiments across three tasks, we demonstrate a clear difference between in-distribution and out-of-distribution, and observe mixed results for interactive explanations: while interactive explanations improve human perception of AI assistance&rsquo;s usefulness, they may reinforce human biases and lead to limited performance improvement. Overall, our work points out critical challenges and future directions towards enhancing human performance with AI assistance."><meta property="og:image" content="https://chenhaot.github.io//images/web_circle_small.jpg"><meta property="twitter:card" content="summary"><meta property="twitter:url" content="https://chenhaot.github.io/papers/ood-interactive-explanation-human-ai-decision-making/"><meta property="twitter:title" content="ood interactive explanation human ai decision making | Welcome to Chenhao Tan's Personal Website!"><meta property="twitter:description" content="Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making
Han Liu, Vivian Lai, and Chenhao Tan.       
In Proceedings of CSCW 2021.
Abstract:
Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in-distribution because of distribution shift, we design experiments with different distribution types and investigate human performance for both in-distribution and out-of-distribution examples. Second, we develop novel interfaces to support interactive explanations so that humans can actively engage with AI assistance. Using virtual pilot studies and large-scale randomized experiments across three tasks, we demonstrate a clear difference between in-distribution and out-of-distribution, and observe mixed results for interactive explanations: while interactive explanations improve human perception of AI assistance&rsquo;s usefulness, they may reinforce human biases and lead to limited performance improvement. Overall, our work points out critical challenges and future directions towards enhancing human performance with AI assistance."><meta property="twitter:image" content="https://chenhaot.github.io//images/web_circle_small.jpg"></head><body><nav><ul class=menu><li><a href=/>Home</a></li><li><a href=/papers/>Papers</a></li><li><a href=/talks/>Talks</a></li><li><a href=/courses/>Courses</a></li><li><a href=/contact/>Contact</a></li><li><a href=https://substack.com/@cichicago>Blog</a></li><li><a href=https://chicagohai.github.io/>Lab</a></li></ul><hr></nav><main><h2 id=understanding-the-effect-of-out-of-distribution-examples-and-interactive-explanations-on-human-ai-decision-making>Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making</h2><p><a href=https://mrsata.github.io/>Han Liu</a>, <a href=https://vivlai.github.io/>Vivian Lai</a>, and <em>Chenhao Tan</em>.<br>In Proceedings of CSCW 2021.</p><p><strong>Abstract:</strong><br>Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in-distribution because of distribution shift, we design experiments with different distribution types and investigate human performance for both in-distribution and out-of-distribution examples. Second, we develop novel interfaces to support interactive explanations so that humans can actively engage with AI assistance. Using virtual pilot studies and large-scale randomized experiments across three tasks, we demonstrate a clear difference between in-distribution and out-of-distribution, and observe mixed results for interactive explanations: while interactive explanations improve human perception of AI assistance&rsquo;s usefulness, they may reinforce human biases and lead to limited performance improvement. Overall, our work points out critical challenges and future directions towards enhancing human performance with AI assistance.</p><p>[<a href=https://arxiv.org/pdf/2101.05303.pdf>PDF</a>]
[<a href=https://youtu.be/HXCAl-ZP8LA>Video</a>]</p><p><img src=https://chenhaot.com/pubs/cscw21.png alt="ood experimental design"></p><p>@inproceedings{liu+lai+tan:21,<br>    
author = {Han Liu and Vivian Lai and Chenhao Tan},<br>    
title = {Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making},<br>    
year = {2021},<br>    
booktitle = {Proceedings of CSCW}<br>}</p></main><footer><hr><p>© Chenhao Tan 2026 (made with <a href=https://github.com/yihui/hugo-xmin/>Hugo XMin</a>)</p><p><a href=https://github.com/ChicagoHAI/>Github</a> | <a href="https://scholar.google.com/citations?user=KGMaP18AAAAJ&amp;hl=en">Google Scholar</a> | <a href=https://x.com/ChenhaoTan>X</a> | <a href=https://bsky.app/profile/chenhaotan.bsky.social>Bluesky</a> | <a href=https://www.linkedin.com/in/chenhao-tan-2a446316/>LinkedIn</a></p></footer></body></html>