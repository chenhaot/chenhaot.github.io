<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Instant Foodie | A minimal Hugo website</title>
<link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><script src=https://kit.fontawesome.com/cdd93c4598.js crossorigin=anonymous></script></head><body><nav><ul class=menu><li><a href=/>Home</a></li><li><a href=/papers/>Papers</a></li><li><a href=/talks/>Talks</a></li><li><a href=/courses/>Courses</a></li><li><a href=/contact/>Contact</a></li><li><a href=/blog/>Blog</a></li><li><a href=https://chicagohai.github.io/>Lab</a></li></ul><hr></nav><main><h2 id=instant-foodie-predicting-expert-ratings-from-grassroots>Instant Foodie: Predicting Expert Ratings From Grassroots</h2><p>Chenhao Tan, <a href=http://www-users.cs.umn.edu/~echi/>Ed H. Chi</a>, <a href=http://www.davehuffaker.com>David Huffaker</a>, <a href=https://sites.google.com/site/gkossinets/>Gueorgi Kossinets</a>, <a href=alex.smola.org>Alexander J. Smola</a><br>In Proceedings of 22nd ACM International Conference on Information and Knowledge Management (CIKM'2013)</p><p>Consumer review sites and recommender systems typically rely on a large volume of user-contributed ratings, which makes rating acquisition an essential component in the design of such systems. User ratings are then summarized to provide an aggregate score representing a popular evaluation of an item. An inherent problem in such summarization is potential bias due to raters’ self-selection and heterogeneity in terms of experiences, tastes and rating scale interpretations. There are two major approaches to collecting ratings, which have different advantages and disadvantages. One is to allow a large number of volunteers to choose and rate items directly (a method employed by e.g. Yelp and Google Places). Alternatively, a panel of raters may be maintained and invited to rate a predefined set of items at regular intervals (such as in Zagat Survey). The latter approach arguably results in more consistent reviews and reduced selection bias, however, at the expense of much smaller coverage (fewer rated items).<br>In this paper, we examine the two different approaches to collecting user ratings of restaurants and explore the question of whether it is possible to reconcile them. Specifically, we study the problem of inferring the more calibrated Zagat Survey ratings (which we dub “expert ratings”) from the user-contributed ratings (“grassroots”) in Google Places. To achieve this, we employ latent factor models and provide a probabilistic treatment of the ordinal ratings. We can predict Zagat Survey ratings accurately from ad hoc usergenerated ratings by employing joint optimization. Furthermore, the resulting model show that users become more discerning as they submit more ratings. We also describe an approach towards cross-city recommendations, answering questions such as “What is the equivalent of the Per Se1 restaurant in Chicago?”</p><p>[<a href=/pubs/cikm-instant-foodie.pdf>Slides</a>][<a href=/pubs/instant-foodie.pdf>PDF</a>]</p><p>@inproceedings{tan+etal:13,<br>    
author = {Chenhao Tan and Ed H. Chi and David Huffaker and Gueorgi Kossinets and Alexander J. Smola},<br>    
title = {Instant Foodie: Predicting Expert Ratings From Grassroots},<br>    
year = {2013},<br>    
booktitle = {Proceedings of CIKM}<br>}</p></main><footer><hr>© Chenhao Tan 2025 (made with <a href=https://github.com/yihui/hugo-xmin/>Hugo XMin</a>) | <a href=https://github.com/ChicagoHAI/>Github</a> | <a href="https://scholar.google.com/citations?user=KGMaP18AAAAJ&amp;hl=en">Google Scholar</a> | <a href=https://bsky.app/profile/chenhaotan.bsky.social>Bluesky</a> | <a href=https://www.linkedin.com/in/chenhao-tan-2a446316/>LinkedIn</a></footer></body></html>