<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>evaluate human rationales | A minimal Hugo website</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><script src=https://kit.fontawesome.com/cdd93c4598.js crossorigin=anonymous></script></head><body><nav><ul class=menu><li><a href=/>Home</a></li><li><a href=/papers/>Papers</a></li><li><a href=/talks/>Talks</a></li><li><a href=/courses/>Courses</a></li><li><a href=/contact/>Contact</a></li><li><a href=/blog/>Blog</a></li><li><a href=https://chicagohai.github.io/>Lab</a></li></ul><hr></nav><main><h2 id=evaluating-and-characterizing-human-rationales>Evaluating and Characterizing Human Rationales</h2><p><a href=https://shcarton.github.io/>Samuel Carton</a>, <a href=https://rathoreanirudh.github.io/>Anirudh Rathore</a>, and <em>Chenhao Tan</em>.<br>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP'2020)</p><p><strong>Abstract:</strong><br>Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using &ldquo;fidelity curves&rdquo; to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.</p><p>[<a href=https://arxiv.org/pdf/2010.04736.pdf>PDF</a>]
[<a href=https://github.com/BoulderDS/evaluating-human-rationales>Code</a>]
[<a href=https://slideslive.com/38939202/evaluating-and-characterizing-human-rationales>Video</a>]</p><p><img src=https://chenhaot.com/pubs/explanations/human_rationale_percentage.png alt="rationale percentage."></p><p>@inproceedings{carton+rathore+tan:20,<br>    
author = {Samuel Carton and Anirudh Rathore and Chenhao Tan},<br>    
title = {Evaluating and Characterizing Human Rationales},<br>    
year = {2020},<br>    
booktitle = {Proceedings of EMNLP}<br>}</p></main><footer><hr>© Chenhao Tan 2025 (made with <a href=https://github.com/yihui/hugo-xmin/>Hugo XMin</a>) | <a href=https://github.com/ChicagoHAI/>Github</a> | <a href="https://scholar.google.com/citations?user=KGMaP18AAAAJ&amp;hl=en">Google Scholar</a> | <a href=https://bsky.app/profile/chenhaotan.bsky.social>Bluesky</a> | <a href=https://www.linkedin.com/in/chenhao-tan-2a446316/>LinkedIn</a></footer></body></html>