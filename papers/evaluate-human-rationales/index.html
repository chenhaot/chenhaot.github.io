<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>evaluate human rationales | Welcome to Chenhao Tan's Personal Website!</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><script src=https://kit.fontawesome.com/cdd93c4598.js crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-QD5VPF7ZFK"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QD5VPF7ZFK")}</script><meta property="og:type" content="website"><meta property="og:url" content="https://chenhaot.github.io/papers/evaluate-human-rationales/"><meta property="og:title" content="evaluate human rationales | Welcome to Chenhao Tan's Personal Website!"><meta property="og:description" content="Evaluating and Characterizing Human Rationales
Samuel Carton, Anirudh Rathore, and Chenhao Tan.      
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP'2020)
Abstract:
Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using &ldquo;fidelity curves&rdquo; to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales."><meta property="og:image" content="https://chenhaot.github.io//images/web_circle_small.jpg"><meta property="twitter:card" content="summary"><meta property="twitter:url" content="https://chenhaot.github.io/papers/evaluate-human-rationales/"><meta property="twitter:title" content="evaluate human rationales | Welcome to Chenhao Tan's Personal Website!"><meta property="twitter:description" content="Evaluating and Characterizing Human Rationales
Samuel Carton, Anirudh Rathore, and Chenhao Tan.      
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP'2020)
Abstract:
Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using &ldquo;fidelity curves&rdquo; to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales."><meta property="twitter:image" content="https://chenhaot.github.io//images/web_circle_small.jpg"></head><body><nav><ul class=menu><li><a href=/>Home</a></li><li><a href=/papers/>Papers</a></li><li><a href=/talks/>Talks</a></li><li><a href=/courses/>Courses</a></li><li><a href=/contact/>Contact</a></li><li><a href=https://substack.com/@cichicago>Blog</a></li><li><a href=https://chicagohai.github.io/>Lab</a></li></ul><hr></nav><main><h2 id=evaluating-and-characterizing-human-rationales>Evaluating and Characterizing Human Rationales</h2><p><a href=https://shcarton.github.io/>Samuel Carton</a>, <a href=https://rathoreanirudh.github.io/>Anirudh Rathore</a>, and <em>Chenhao Tan</em>.<br>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP'2020)</p><p><strong>Abstract:</strong><br>Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using &ldquo;fidelity curves&rdquo; to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.</p><p>[<a href=https://arxiv.org/pdf/2010.04736.pdf>PDF</a>]
[<a href=https://github.com/BoulderDS/evaluating-human-rationales>Code</a>]
[<a href=https://slideslive.com/38939202/evaluating-and-characterizing-human-rationales>Video</a>]</p><p><img src=https://chenhaot.com/pubs/explanations/human_rationale_percentage.png alt="rationale percentage."></p><p>@inproceedings{carton+rathore+tan:20,<br>    
author = {Samuel Carton and Anirudh Rathore and Chenhao Tan},<br>    
title = {Evaluating and Characterizing Human Rationales},<br>    
year = {2020},<br>    
booktitle = {Proceedings of EMNLP}<br>}</p></main><footer><hr><p>© Chenhao Tan 2025 (made with <a href=https://github.com/yihui/hugo-xmin/>Hugo XMin</a>)</p><p><a href=https://github.com/ChicagoHAI/>Github</a> | <a href="https://scholar.google.com/citations?user=KGMaP18AAAAJ&amp;hl=en">Google Scholar</a> | <a href=https://x.com/ChenhaoTan>X</a> | <a href=https://bsky.app/profile/chenhaotan.bsky.social>Bluesky</a> | <a href=https://www.linkedin.com/in/chenhao-tan-2a446316/>LinkedIn</a></p></footer></body></html>