<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>entity lm | Welcome to Chenhao Tan's Personal Website!</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><script src=https://kit.fontawesome.com/cdd93c4598.js crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-QD5VPF7ZFK"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QD5VPF7ZFK")}</script></head><body><nav><ul class=menu><li><a href=/>Home</a></li><li><a href=/papers/>Papers</a></li><li><a href=/talks/>Talks</a></li><li><a href=/courses/>Courses</a></li><li><a href=/contact/>Contact</a></li><li><a href=https://substack.com/@cichicago>Blog</a></li><li><a href=https://chicagohai.github.io/>Lab</a></li></ul><hr></nav><main><h2 id=dynamic-entity-representations-in-neural-language-models>Dynamic Entity Representations in Neural Language Models</h2><p><em><a href=http://jiyfeng.github.io/>Yangfeng Ji</a>, Chenhao Tan, <a href=http://smartschat.de/>Sebastian Martschat</a>, <a href=https://homes.cs.washington.edu/~yejin/>Yejin Choi</a>, <a href=http://homes.cs.washington.edu/~nasmith/>Noah A. Smith</a></em><br>In Proceedings of EMNLP 2017</p><p><strong>Abstract:</strong><br>Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, ENTITYNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.</p><p>[<a href=/pubs/entity-language-model.pdf>PDF</a>]
[<a href=https://github.com/jiyfeng/entitynlm>Code</a>]</p><p>@inproceedings{ji+etal:17,<br>    
author = {Yangfeng Ji and Chenhao Tan and Sebastian Martschat and Yejin Choi and Noah A. Smith},<br>    
title = {Dynamic Entity Representations in Neural Language Models},<br>    
year = {2017},<br>    
booktitle = {Proceedings of EMNLP}<br>}</p></main><footer><hr>© Chenhao Tan 2025 (made with <a href=https://github.com/yihui/hugo-xmin/>Hugo XMin</a>) | <a href=https://github.com/ChicagoHAI/>Github</a> | <a href="https://scholar.google.com/citations?user=KGMaP18AAAAJ&amp;hl=en">Google Scholar</a> | <a href=https://x.com/ChenhaoTan>X</a> | <a href=https://bsky.app/profile/chenhaotan.bsky.social>Bluesky</a> | <a href=https://www.linkedin.com/in/chenhao-tan-2a446316/>LinkedIn</a></footer></body></html>