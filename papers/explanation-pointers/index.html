<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>explanation pointers | A minimal Hugo website</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><script src=https://kit.fontawesome.com/cdd93c4598.js crossorigin=anonymous></script></head><body><nav><ul class=menu><li><a href=/>Home</a></li><li><a href=/papers/>Papers</a></li><li><a href=/talks/>Talks</a></li><li><a href=/courses/>Courses</a></li><li><a href=/contact/>Contact</a></li><li><a href=/blog/>Blog</a></li><li><a href=https://chicagohai.github.io/>Lab</a></li></ul><hr></nav><main><h2 id=what-gets-echoed-understanding-the-pointers-in-explanations-of-persuasive-arguments>What Gets Echoed? Understanding the &lsquo;Pointers&rsquo; in Explanations of Persuasive Arguments</h2><p>David Atkinson, Kumar Bhargav Srinivasan, and <em>Chenhao Tan</em>.<br>In Proceedings of 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP'2019)</p><p><strong>Abstract:</strong><br>Explanations are central to everyday life, and are a topic of growing interest in the AI community. To investigate the process of providing natural language explanations, we leverage the dynamics of the /r/ChangeMyView subreddit to build a dataset with 36K naturally occurring explanations of why an argument is persuasive. We propose a novel word-level prediction task to investigate how explanations selectively reuse, or echo, information from what is being explained (henceforth, explanandum). We develop features to capture the properties of a word in the explanandum, and show that our proposed features not only have relatively strong predictive power on the echoing of a word in an explanation, but also enhance neural methods of generating explanations. In particular, while the non-contextual properties of a word itself are more valuable for stopwords, the interaction between the constituent parts of an explanandum is crucial in predicting the echoing of content words. We also find intriguing patterns of a word being echoed. For example, although nouns are generally less likely to be echoed, subjects and objects can, depending on their source, be more likely to be echoed in the explanations.</p><p>[<a href=https://chenhaot.com/pubs/explanations/explanation-pointers.pdf>PDF</a>]
[<a href=https://github.com/davatk/what-gets-echoed/>Code & Data</a>][<a href=/pubs/explanations/explanation-pointer-slides.pdf>Slides</a>]</p><p><img src=https://chenhaot.com/pubs/explanations/explanations-echoing-explananda.png alt="echoing probability."></p><p>@inproceedings{atkinson+srinivasan+tan:19,<br>    
author = {David Atkinson and Kumar Bhargav Srinivasan and Chenhao Tan},<br>    
title = {What Gets Echoed? Understanding the &lsquo;Pointers&rsquo; in Explanations of Persuasive Arguments},<br>    
year = {2019},<br>    
booktitle = {Proceedings of EMNLP}<br>}</p></main><footer><hr>© Chenhao Tan 2025 (made with <a href=https://github.com/yihui/hugo-xmin/>Hugo XMin</a>) | <a href=https://github.com/ChicagoHAI/>Github</a> | <a href="https://scholar.google.com/citations?user=KGMaP18AAAAJ&amp;hl=en">Google Scholar</a> | <a href=https://bsky.app/profile/chenhaotan.bsky.social>Bluesky</a> | <a href=https://www.linkedin.com/in/chenhao-tan-2a446316/>LinkedIn</a></footer></body></html>